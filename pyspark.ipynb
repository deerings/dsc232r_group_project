{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d1daa8c-f894-4fee-b74b-2f8ab6b70635",
   "metadata": {},
   "source": [
    "# FEMA's National Flood Insurance Policy Database Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d6da5-22f1-4bde-8d4f-34a969ed9394",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13acd25f-90fc-48eb-823f-370db3aa29af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, ByteType, LongType, FloatType, ShortType\n",
    "from pyspark.sql.functions import col, sum as spark_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81025ff1-9dae-4f7a-857e-90a64d6e4179",
   "metadata": {},
   "source": [
    "### Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf8b391c-8562-45da-984f-f50a7ec876ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 12:55:30 WARN Utils: Your hostname, MK-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.39 instead (on interface en0)\n",
      "24/05/28 12:55:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/28 12:55:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/28 12:55:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "\t.config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config('spark.executor.instances', 5) \\\n",
    "\t.config(\"spark.sql.debug.maxToStringFields\", \"100\")\\\n",
    "    .appName(\"Flood Data\") \\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7918cf43-bfd7-44fd-b72a-ae3fa6b39772",
   "metadata": {},
   "source": [
    "### Load data to Spark dataframe; Infer Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591ba87c-16da-4351-aa84-0c05d5d85139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 12:55:43 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#path to data \n",
    "data_path = \"./NFIP/nfip-flood-policies.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c74f6e-4b75-4be6-b2a9-8ed7c76216c0",
   "metadata": {},
   "source": [
    "## Perform the Data Exploration\n",
    "Below is the outline of the Data Exploration Section:\n",
    "- Print Schema\n",
    "- Column Description\n",
    "- Display the Number of Variables (Columnns)\n",
    "- Show the First Few Rows\n",
    "- Display Number of Observations\n",
    "- Missing Values in the Dataframe\n",
    "- Statistics Summary and Data Distribution\n",
    "- Correlations Among Variables\n",
    "- Scatterplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c2c7c",
   "metadata": {},
   "source": [
    "### Print Schema\n",
    "\n",
    "Understanding the structure of data (column names and types) is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d45cb2-cccb-43e1-bfba-809b1aa61897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- agriculturestructureindicator: string (nullable = true)\n",
      " |-- basefloodelevation: double (nullable = true)\n",
      " |-- basementenclosurecrawlspacetype: integer (nullable = true)\n",
      " |-- cancellationdateoffloodpolicy: date (nullable = true)\n",
      " |-- censustract: long (nullable = true)\n",
      " |-- condominiumindicator: string (nullable = true)\n",
      " |-- construction: string (nullable = true)\n",
      " |-- countycode: integer (nullable = true)\n",
      " |-- crsdiscount: double (nullable = true)\n",
      " |-- deductibleamountinbuildingcoverage: string (nullable = true)\n",
      " |-- deductibleamountincontentscoverage: string (nullable = true)\n",
      " |-- elevatedbuildingindicator: string (nullable = true)\n",
      " |-- elevationcertificateindicator: string (nullable = true)\n",
      " |-- elevationdifference: integer (nullable = true)\n",
      " |-- federalpolicyfee: integer (nullable = true)\n",
      " |-- floodzone: string (nullable = true)\n",
      " |-- hfiaasurcharge: integer (nullable = true)\n",
      " |-- houseofworshipindicator: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- locationofcontents: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- lowestadjacentgrade: double (nullable = true)\n",
      " |-- lowestfloorelevation: double (nullable = true)\n",
      " |-- nonprofitindicator: string (nullable = true)\n",
      " |-- numberoffloorsininsuredbuilding: integer (nullable = true)\n",
      " |-- obstructiontype: string (nullable = true)\n",
      " |-- occupancytype: integer (nullable = true)\n",
      " |-- originalconstructiondate: date (nullable = true)\n",
      " |-- originalnbdate: date (nullable = true)\n",
      " |-- policycost: integer (nullable = true)\n",
      " |-- policycount: integer (nullable = true)\n",
      " |-- policyeffectivedate: date (nullable = true)\n",
      " |-- policyterminationdate: date (nullable = true)\n",
      " |-- policytermindicator: integer (nullable = true)\n",
      " |-- postfirmconstructionindicator: string (nullable = true)\n",
      " |-- primaryresidenceindicator: string (nullable = true)\n",
      " |-- propertystate: string (nullable = true)\n",
      " |-- reportedzipcode: string (nullable = true)\n",
      " |-- ratemethod: string (nullable = true)\n",
      " |-- regularemergencyprogramindicator: string (nullable = true)\n",
      " |-- reportedcity: string (nullable = true)\n",
      " |-- smallbusinessindicatorbuilding: string (nullable = true)\n",
      " |-- totalbuildinginsurancecoverage: integer (nullable = true)\n",
      " |-- totalcontentsinsurancecoverage: integer (nullable = true)\n",
      " |-- totalinsurancepremiumofthepolicy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06482799",
   "metadata": {},
   "source": [
    "### Column Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a5dbb",
   "metadata": {},
   "source": [
    "**Description of the columns from the FEMA's National Flood Insurance Policy Database, grouped by their data types and purpose:**\n",
    "\n",
    "**Geographic and Location Details**\n",
    "- **censustract (long):** Census tract number indicating the specific area where the property is located, used for demographic analysis.\n",
    "- **countycode (integer):** Numeric code representing the county in which the property is insured.\n",
    "- **floodzone (string):** Designation of the flood zone according to FEMA's mapping, crucial for assessing the property's flood risk.\n",
    "- **latitude (double), longitude (double):** Geographic coordinates specifying the precise location of the property.\n",
    "- **propertystate (string):** The U.S. state where the property is located.\n",
    "- **reportedcity (string):** The city reported for the insured property.\n",
    "- **reportedzipcode (integer):** Zip code where the property is situated, used for localizing insurance coverage and risk.\n",
    "\n",
    "**Property and Construction Details**\n",
    "- **agriculturestructureindicator (string):** Indicates whether the property is used for agricultural purposes.\n",
    "- **basementenclosurecrawlspacetype (integer):** Type of basement or crawlspace present at the property, affecting flood risk assessment.\n",
    "- **construction (string):** Describes the type of construction materials and methods used, which can affect the property's vulnerability to flood damage.\n",
    "- **numberoffloorsininsuredbuilding (integer):** Total floors in the insured building, important for determining potential flood damage and insurance coverage needs.\n",
    "- **elevatedbuildingindicator (string):** Indicates whether the building is elevated, a key factor in reducing flood risk.\n",
    "\n",
    "**Policy Details**\n",
    "- **policycost (integer):** The total cost of the flood insurance policy.\n",
    "- **policycount (integer):** The number of policies associated with a single property or account.\n",
    "- **policyeffectivedate (date), policyterminationdate (date):** Start and end dates of the flood insurance coverage.\n",
    "- **totalbuildinginsurancecoverage (integer), totalcontentsinsurancecoverage (integer):** The amount of insurance coverage for the building and its contents, respectively.\n",
    "- **totalinsurancepremiumofthepolicy (integer):** Total premium amount for the flood insurance policy.\n",
    "\n",
    "**Flood Risk Assessment Specifics**\n",
    "- **basefloodelevation (double):** The base flood elevation expected for a particular area, critical for understanding flood risk levels.\n",
    "- **elevationcertificateindicator (string), elevationdifference (integer):** Presence of an elevation certificate and the difference in elevation, respectively, both crucial for assessing compliance with floodplain management.\n",
    "- **lowestadjacentgrade (double), lowestfloorelevation (double):** Measures of elevation that help determine the property's flood exposure.\n",
    "\n",
    "**Insurance Policy Features**\n",
    "- **crsdiscount (double):** Community Rating System discount applied to the policy, which can reduce insurance premiums based on community flood preparedness.\n",
    "- **deductibleamountinbuildingcoverage (integer), deductibleamountincontentscoverage (integer):** Deductible amounts for building and contents coverage, influencing out-of-pocket costs after a flood.\n",
    "- **hfiaasurcharge (integer):** Surcharge applied under the Homeowner Flood Insurance Affordability Act.\n",
    "- **federalpolicyfee (integer):** A fee associated with the federal policy governing flood insurance.\n",
    "\n",
    "**Special Indicators**\n",
    "- **condominiumindicator (string), primaryresidenceindicator (string):** Indicate whether the insured property is a condominium or the primary residence of the owner.\n",
    "- **houseofworshipindicator (string), nonprofitindicator (string):** Indicators of whether the property is used as a house of worship or is owned by a nonprofit organization, affecting policy terms and possibly qualifying for special considerations.\n",
    "- **postfirmconstructionindicator (string):** Indicates if the building was constructed after the community's first Flood Insurance Rate Map was issued, which can affect insurance rates.\n",
    "- **smallbusinessindicatorbuilding (string):** Indicates whether the insured building is used for small business purposes.\n",
    "\n",
    "**Additional Policy and Coverage Information**\n",
    "- **originalconstructiondate (date), originalnbdate (date):** Dates of original construction and the building's initial notebook entry, important for historical property assessments.\n",
    "- **cancellationdateoffloodpolicy (date):** Date when the flood policy was cancelled, if applicable.\n",
    "- **regularemergencyprogramindicator (string):** Indicates the type of FEMA program under which the policy is covered, distinguishing between regular and emergency management programs.\n",
    "- **ratemethod (integer):** Describes the method used to calculate the insurance rate, impacting how premiums are determined.\n",
    "- **locationofcontents (string):** Specifies where within the property the insured contents are located, relevant for claims and risk assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79314eec",
   "metadata": {},
   "source": [
    "### Display number of Variables (Columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2067f5ad-5bd4-4c10-920e-554682945ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns: 45\n"
     ]
    }
   ],
   "source": [
    "num_variables = len(df.columns)\n",
    "print(\"Number of Columns:\", num_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de894071",
   "metadata": {},
   "source": [
    "### Show the First Few Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc644f3-a9f6-4f22-8b3b-eee3f90b4e3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------\n",
      " agriculturestructureindicator      | NULL                 \n",
      " basefloodelevation                 | NULL                 \n",
      " basementenclosurecrawlspacetype    | 2                    \n",
      " cancellationdateoffloodpolicy      | NULL                 \n",
      " censustract                        | 33013038500          \n",
      " condominiumindicator               | N                    \n",
      " construction                       | N                    \n",
      " countycode                         | 33013                \n",
      " crsdiscount                        | 0.0                  \n",
      " deductibleamountinbuildingcoverage | 0                    \n",
      " deductibleamountincontentscoverage | 0                    \n",
      " elevatedbuildingindicator          | N                    \n",
      " elevationcertificateindicator      | NULL                 \n",
      " elevationdifference                | 999                  \n",
      " federalpolicyfee                   | 13                   \n",
      " floodzone                          | X                    \n",
      " hfiaasurcharge                     | 0                    \n",
      " houseofworshipindicator            | NULL                 \n",
      " latitude                           | 43.3                 \n",
      " locationofcontents                 | Basement/Enclosur... \n",
      " longitude                          | -71.8                \n",
      " lowestadjacentgrade                | NULL                 \n",
      " lowestfloorelevation               | NULL                 \n",
      " nonprofitindicator                 | NULL                 \n",
      " numberoffloorsininsuredbuilding    | 2                    \n",
      " obstructiontype                    | NULL                 \n",
      " occupancytype                      | 1                    \n",
      " originalconstructiondate           | 1974-12-30           \n",
      " originalnbdate                     | 2008-08-19           \n",
      " policycost                         | 388                  \n",
      " policycount                        | 1                    \n",
      " policyeffectivedate                | 2009-08-19           \n",
      " policyterminationdate              | 2010-08-19           \n",
      " policytermindicator                | 1                    \n",
      " postfirmconstructionindicator      | N                    \n",
      " primaryresidenceindicator          | Y                    \n",
      " propertystate                      | NH                   \n",
      " reportedzipcode                    | 03278                \n",
      " ratemethod                         | 7                    \n",
      " regularemergencyprogramindicator   | R                    \n",
      " reportedcity                       | WARNER               \n",
      " smallbusinessindicatorbuilding     | NULL                 \n",
      " totalbuildinginsurancecoverage     | 250000               \n",
      " totalcontentsinsurancecoverage     | 100000               \n",
      " totalinsurancepremiumofthepolicy   | 375                  \n",
      "-RECORD 1--------------------------------------------------\n",
      " agriculturestructureindicator      | NULL                 \n",
      " basefloodelevation                 | NULL                 \n",
      " basementenclosurecrawlspacetype    | 0                    \n",
      " cancellationdateoffloodpolicy      | NULL                 \n",
      " censustract                        | 22063040700          \n",
      " condominiumindicator               | N                    \n",
      " construction                       | N                    \n",
      " countycode                         | 22063                \n",
      " crsdiscount                        | 0.05                 \n",
      " deductibleamountinbuildingcoverage | 1                    \n",
      " deductibleamountincontentscoverage | 1                    \n",
      " elevatedbuildingindicator          | N                    \n",
      " elevationcertificateindicator      | NULL                 \n",
      " elevationdifference                | 999                  \n",
      " federalpolicyfee                   | 35                   \n",
      " floodzone                          | AE                   \n",
      " hfiaasurcharge                     | 0                    \n",
      " houseofworshipindicator            | NULL                 \n",
      " latitude                           | 30.5                 \n",
      " locationofcontents                 | Lowest floor only... \n",
      " longitude                          | -91.0                \n",
      " lowestadjacentgrade                | NULL                 \n",
      " lowestfloorelevation               | NULL                 \n",
      " nonprofitindicator                 | NULL                 \n",
      " numberoffloorsininsuredbuilding    | 1                    \n",
      " obstructiontype                    | NULL                 \n",
      " occupancytype                      | 3                    \n",
      " originalconstructiondate           | 1974-07-01           \n",
      " originalnbdate                     | 1997-10-04           \n",
      " policycost                         | 315                  \n",
      " policycount                        | 1                    \n",
      " policyeffectivedate                | 2009-10-04           \n",
      " policyterminationdate              | 2010-10-04           \n",
      " policytermindicator                | 1                    \n",
      " postfirmconstructionindicator      | N                    \n",
      " primaryresidenceindicator          | Y                    \n",
      " propertystate                      | LA                   \n",
      " reportedzipcode                    | 70726                \n",
      " ratemethod                         | 1                    \n",
      " regularemergencyprogramindicator   | R                    \n",
      " reportedcity                       | DENHAM SPRINGS       \n",
      " smallbusinessindicatorbuilding     | NULL                 \n",
      " totalbuildinginsurancecoverage     | 16400                \n",
      " totalcontentsinsurancecoverage     | 8800                 \n",
      " totalinsurancepremiumofthepolicy   | 280                  \n",
      "-RECORD 2--------------------------------------------------\n",
      " agriculturestructureindicator      | NULL                 \n",
      " basefloodelevation                 | NULL                 \n",
      " basementenclosurecrawlspacetype    | 0                    \n",
      " cancellationdateoffloodpolicy      | NULL                 \n",
      " censustract                        | 45051060204          \n",
      " condominiumindicator               | N                    \n",
      " construction                       | N                    \n",
      " countycode                         | 45051                \n",
      " crsdiscount                        | 0.0                  \n",
      " deductibleamountinbuildingcoverage | 0                    \n",
      " deductibleamountincontentscoverage | 0                    \n",
      " elevatedbuildingindicator          | N                    \n",
      " elevationcertificateindicator      | 1                    \n",
      " elevationdifference                | 999                  \n",
      " federalpolicyfee                   | 13                   \n",
      " floodzone                          | X                    \n",
      " hfiaasurcharge                     | 0                    \n",
      " houseofworshipindicator            | NULL                 \n",
      " latitude                           | 33.7                 \n",
      " locationofcontents                 | Lowest floor only... \n",
      " longitude                          | -79.0                \n",
      " lowestadjacentgrade                | NULL                 \n",
      " lowestfloorelevation               | NULL                 \n",
      " nonprofitindicator                 | NULL                 \n",
      " numberoffloorsininsuredbuilding    | 1                    \n",
      " obstructiontype                    | NULL                 \n",
      " occupancytype                      | 1                    \n",
      " originalconstructiondate           | 2003-07-01           \n",
      " originalnbdate                     | 2005-08-13           \n",
      " policycost                         | 348                  \n",
      " policycount                        | 1                    \n",
      " policyeffectivedate                | 2009-08-13           \n",
      " policyterminationdate              | 2010-08-13           \n",
      " policytermindicator                | 1                    \n",
      " postfirmconstructionindicator      | Y                    \n",
      " primaryresidenceindicator          | Y                    \n",
      " propertystate                      | SC                   \n",
      " reportedzipcode                    | 29579                \n",
      " ratemethod                         | 7                    \n",
      " regularemergencyprogramindicator   | R                    \n",
      " reportedcity                       | MYRTLE BEACH         \n",
      " smallbusinessindicatorbuilding     | NULL                 \n",
      " totalbuildinginsurancecoverage     | 250000               \n",
      " totalcontentsinsurancecoverage     | 100000               \n",
      " totalinsurancepremiumofthepolicy   | 335                  \n",
      "-RECORD 3--------------------------------------------------\n",
      " agriculturestructureindicator      | NULL                 \n",
      " basefloodelevation                 | 519.0                \n",
      " basementenclosurecrawlspacetype    | 2                    \n",
      " cancellationdateoffloodpolicy      | 2016-04-01           \n",
      " censustract                        | 1055001200           \n",
      " condominiumindicator               | N                    \n",
      " construction                       | N                    \n",
      " countycode                         | 1055                 \n",
      " crsdiscount                        | 0.0                  \n",
      " deductibleamountinbuildingcoverage | 1                    \n",
      " deductibleamountincontentscoverage | 1                    \n",
      " elevatedbuildingindicator          | Y                    \n",
      " elevationcertificateindicator      | NULL                 \n",
      " elevationdifference                | -2                   \n",
      " federalpolicyfee                   | 35                   \n",
      " floodzone                          | AE                   \n",
      " hfiaasurcharge                     | 0                    \n",
      " houseofworshipindicator            | NULL                 \n",
      " latitude                           | 34.0                 \n",
      " locationofcontents                 | NULL                 \n",
      " longitude                          | -86.0                \n",
      " lowestadjacentgrade                | 517.3                \n",
      " lowestfloorelevation               | 517.3                \n",
      " nonprofitindicator                 | NULL                 \n",
      " numberoffloorsininsuredbuilding    | 3                    \n",
      " obstructiontype                    | 50                   \n",
      " occupancytype                      | 1                    \n",
      " originalconstructiondate           | 1989-01-01           \n",
      " originalnbdate                     | 2006-04-14           \n",
      " policycost                         | 951                  \n",
      " policycount                        | 1                    \n",
      " policyeffectivedate                | 2009-04-14           \n",
      " policyterminationdate              | 2010-04-14           \n",
      " policytermindicator                | 1                    \n",
      " postfirmconstructionindicator      | Y                    \n",
      " primaryresidenceindicator          | Y                    \n",
      " propertystate                      | AL                   \n",
      " reportedzipcode                    | 35901                \n",
      " ratemethod                         | 2                    \n",
      " regularemergencyprogramindicator   | R                    \n",
      " reportedcity                       | GADSDEN              \n",
      " smallbusinessindicatorbuilding     | NULL                 \n",
      " totalbuildinginsurancecoverage     | 174900               \n",
      " totalcontentsinsurancecoverage     | 21000                \n",
      " totalinsurancepremiumofthepolicy   | 916                  \n",
      "-RECORD 4--------------------------------------------------\n",
      " agriculturestructureindicator      | NULL                 \n",
      " basefloodelevation                 | 7.0                  \n",
      " basementenclosurecrawlspacetype    | 0                    \n",
      " cancellationdateoffloodpolicy      | NULL                 \n",
      " censustract                        | 12086000115          \n",
      " condominiumindicator               | N                    \n",
      " construction                       | N                    \n",
      " countycode                         | 12086                \n",
      " crsdiscount                        | 0.0                  \n",
      " deductibleamountinbuildingcoverage | 0                    \n",
      " deductibleamountincontentscoverage | 0                    \n",
      " elevatedbuildingindicator          | N                    \n",
      " elevationcertificateindicator      | 3                    \n",
      " elevationdifference                | 0                    \n",
      " federalpolicyfee                   | 35                   \n",
      " floodzone                          | A10                  \n",
      " hfiaasurcharge                     | 0                    \n",
      " houseofworshipindicator            | NULL                 \n",
      " latitude                           | 26.0                 \n",
      " locationofcontents                 | Lowest floor only... \n",
      " longitude                          | -80.1                \n",
      " lowestadjacentgrade                | NULL                 \n",
      " lowestfloorelevation               | 6.9                  \n",
      " nonprofitindicator                 | NULL                 \n",
      " numberoffloorsininsuredbuilding    | 1                    \n",
      " obstructiontype                    | NULL                 \n",
      " occupancytype                      | 1                    \n",
      " originalconstructiondate           | 1957-07-01           \n",
      " originalnbdate                     | 1996-04-04           \n",
      " policycost                         | 1323                 \n",
      " policycount                        | 1                    \n",
      " policyeffectivedate                | 2009-04-04           \n",
      " policyterminationdate              | 2010-04-04           \n",
      " policytermindicator                | 1                    \n",
      " postfirmconstructionindicator      | N                    \n",
      " primaryresidenceindicator          | Y                    \n",
      " propertystate                      | FL                   \n",
      " reportedzipcode                    | 33160                \n",
      " ratemethod                         | 1                    \n",
      " regularemergencyprogramindicator   | R                    \n",
      " reportedcity                       | GOLDEN BEACH         \n",
      " smallbusinessindicatorbuilding     | NULL                 \n",
      " totalbuildinginsurancecoverage     | 250000               \n",
      " totalcontentsinsurancecoverage     | 100000               \n",
      " totalinsurancepremiumofthepolicy   | 1288                 \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541ea90",
   "metadata": {},
   "source": [
    "### Display Number of Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f37f095-f79b-47a4-9761-6a0a220ffa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50406943"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ea34b",
   "metadata": {},
   "source": [
    "FEMA's National Flood Insurance Policy Database, containing over 50 million (50,406,943) policy transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46603b6",
   "metadata": {},
   "source": [
    "### Missing Values in the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c77deb-7d94-471c-85c3-ceea1fa9ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find number of missing values in the dataframe\n",
    "missing_vals = df.select(*(spark_sum(col(i).isNull().cast(\"int\")).alias(i) for i in df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6554286b-9bcf-4465-964f-f0431fbdb0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=====================================================>   (91 + 6) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " agriculturestructureindicator      | 38923313 \n",
      " basefloodelevation                 | 33636759 \n",
      " basementenclosurecrawlspacetype    | 802      \n",
      " cancellationdateoffloodpolicy      | 43614057 \n",
      " censustract                        | 467119   \n",
      " condominiumindicator               | 6        \n",
      " construction                       | 13       \n",
      " countycode                         | 48999    \n",
      " crsdiscount                        | 0        \n",
      " deductibleamountinbuildingcoverage | 661993   \n",
      " deductibleamountincontentscoverage | 5561584  \n",
      " elevatedbuildingindicator          | 258      \n",
      " elevationcertificateindicator      | 32606397 \n",
      " elevationdifference                | 0        \n",
      " federalpolicyfee                   | 0        \n",
      " floodzone                          | 169145   \n",
      " hfiaasurcharge                     | 0        \n",
      " houseofworshipindicator            | 34476251 \n",
      " latitude                           | 338699   \n",
      " locationofcontents                 | 15389767 \n",
      " longitude                          | 338699   \n",
      " lowestadjacentgrade                | 34940579 \n",
      " lowestfloorelevation               | 33060602 \n",
      " nonprofitindicator                 | 34493094 \n",
      " numberoffloorsininsuredbuilding    | 162301   \n",
      " obstructiontype                    | 40629070 \n",
      " occupancytype                      | 6        \n",
      " originalconstructiondate           | 180318   \n",
      " originalnbdate                     | 0        \n",
      " policycost                         | 0        \n",
      " policycount                        | 0        \n",
      " policyeffectivedate                | 0        \n",
      " policyterminationdate              | 0        \n",
      " policytermindicator                | 3        \n",
      " postfirmconstructionindicator      | 180276   \n",
      " primaryresidenceindicator          | 21884    \n",
      " propertystate                      | 0        \n",
      " reportedzipcode                    | 7        \n",
      " ratemethod                         | 902967   \n",
      " regularemergencyprogramindicator   | 2        \n",
      " reportedcity                       | 2        \n",
      " smallbusinessindicatorbuilding     | 33451148 \n",
      " totalbuildinginsurancecoverage     | 0        \n",
      " totalcontentsinsurancecoverage     | 0        \n",
      " totalinsurancepremiumofthepolicy   | 0        \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "missing_vals.show(vertical= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c68f1",
   "metadata": {},
   "source": [
    "**Here's a description of the missing value situation in the FEMA Flood Insurance Policy Database:** \n",
    "\n",
    "1. **High Missing Values:**\n",
    "- **Base Flood Elevation, Latitude, Longitude, Lowest Adjacent Grade, Lowest Floor Elevation:** These fields each have around 50,406,943 missing values. This suggests a significant lack of geographic and elevation data, which are critical in flood insurance calculations.\n",
    "- **Elevation Certificate Indicator, Elevation Difference:** Both fields are missing approximately 32,806,397 and 32,897,994 values respectively, indicating that elevation certificates, which are vital for verifying compliance with floodplain management regulations, are largely absent.\n",
    "- **Obstruction Type:** Missing around 40,793,526 values, indicating that details about obstructions which can affect flood risk assessments are predominantly not reported.\n",
    "- **House of Worship Indicator, Nonprofit Indicator:** Each has over 34,476,251 and 34,493,094 missing entries respectively, indicating a lack of identification of these property types, which might have different considerations in policy terms.\n",
    "\n",
    "2. **Moderate Missing Values:**\n",
    "- **Deductible Amount in Building Coverage, Deductible Amount in Contents Coverage:** Missing 15,649,149 and 18,265,104 values respectively, which implies incomplete data on policy deductibles that could affect premium calculations and risk assessments.\n",
    "- **Location of Contents:** With 15,389,767 missing entries, there's substantial missing information on where contents are located within the insured buildings, which is vital for damage assessments.\n",
    "\n",
    "3. **Low Missing Values:**\n",
    "- **Census Tract, Flood Zone:** Missing 467,119 and 169,145 entries respectively. Although relatively lower, these still represent significant gaps, especially as these fields are crucial for location-specific risk assessment.\n",
    "- **Number of Floors in Insured Building:** Missing data on 162,301 entries could affect understanding building structure and associated risk.\n",
    "\n",
    "4. **Minimal to No Missing Values:**\n",
    "- Fields like **CRS Discount, Federal Policy Fee**, and various policy-related dates (effectiveness, termination) and costs show zero missing values, indicating complete data in terms of policy transaction details.\n",
    "- Similarly, **County Code, Construction, Condominium Indicator, Occupancy Type** show minimal missing data (under 50,000), suggesting good coverage of basic property and policyholder information.\n",
    "\n",
    "Overall, the dataset shows a strong presence of policy and basic property information but suffers from a significant absence of detailed geographic and structural data. This gap in data can hamper effective risk assessment and pricing of flood insurance policies, especially in areas prone to flooding where such data is most critical. Addressing these missing values, either by data imputation where appropriate or by collecting missing data, could significantly enhance the robustness of any analysis or predictive modeling based on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0471c",
   "metadata": {},
   "source": [
    "### Statistics Summary and Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8760f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 13:00:34 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:========================================================>(96 + 1) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------\n",
      " summary                          | count                \n",
      " basefloodelevation               | 16770184             \n",
      " basementenclosurecrawlspacetype  | 50406141             \n",
      " censustract                      | 49939824             \n",
      " countycode                       | 50357944             \n",
      " crsdiscount                      | 50406943             \n",
      " elevationdifference              | 50406943             \n",
      " federalpolicyfee                 | 50406943             \n",
      " hfiaasurcharge                   | 50406943             \n",
      " latitude                         | 50068244             \n",
      " longitude                        | 50068244             \n",
      " lowestadjacentgrade              | 15466364             \n",
      " lowestfloorelevation             | 17346341             \n",
      " numberoffloorsininsuredbuilding  | 50244642             \n",
      " occupancytype                    | 50406937             \n",
      " policycost                       | 50406943             \n",
      " policycount                      | 50406943             \n",
      " policytermindicator              | 50406940             \n",
      " totalbuildinginsurancecoverage   | 50406943             \n",
      " totalcontentsinsurancecoverage   | 50406943             \n",
      " totalinsurancepremiumofthepolicy | 50406943             \n",
      "-RECORD 1------------------------------------------------\n",
      " summary                          | mean                 \n",
      " basefloodelevation               | 119.46875184553842   \n",
      " basementenclosurecrawlspacetype  | 0.37034783122953213  \n",
      " censustract                      | 2.600713413696665E10 \n",
      " countycode                       | 26131.694903946038   \n",
      " crsdiscount                      | 0.0638045804523593   \n",
      " elevationdifference              | 652.7287821044811    \n",
      " federalpolicyfee                 | 37.28163846397113    \n",
      " hfiaasurcharge                   | 35.978938476788805   \n",
      " latitude                         | 32.56980881534298    \n",
      " longitude                        | -87.84068065981351   \n",
      " lowestadjacentgrade              | 129.1970600006573    \n",
      " lowestfloorelevation             | 385.6220114835789    \n",
      " numberoffloorsininsuredbuilding  | 1.6506924061674078   \n",
      " occupancytype                    | 1.3606187576920217   \n",
      " policycost                       | 912.2705097787818    \n",
      " policycount                      | 1.2533995961627746   \n",
      " policytermindicator              | 1.0064443507183733   \n",
      " totalbuildinginsurancecoverage   | 237988.65857626003   \n",
      " totalcontentsinsurancecoverage   | 56495.85875104547    \n",
      " totalinsurancepremiumofthepolicy | 782.522127437881     \n",
      "-RECORD 2------------------------------------------------\n",
      " summary                          | stddev               \n",
      " basefloodelevation               | 522.4923899327453    \n",
      " basementenclosurecrawlspacetype  | 0.8565172820726049   \n",
      " censustract                      | 1.57505783815547E10  \n",
      " countycode                       | 15890.818432604121   \n",
      " crsdiscount                      | 0.09100289197081944  \n",
      " elevationdifference              | 477.66093269871527   \n",
      " federalpolicyfee                 | 74.43304422666706    \n",
      " hfiaasurcharge                   | 79.39438961634104    \n",
      " latitude                         | 5.783491098896586    \n",
      " longitude                        | 13.358819579013812   \n",
      " lowestadjacentgrade              | 609.9164851452954    \n",
      " lowestfloorelevation             | 1676.4233283396445   \n",
      " numberoffloorsininsuredbuilding  | 0.8270384564297135   \n",
      " occupancytype                    | 0.9757148873746709   \n",
      " policycost                       | 1808.6990867989805   \n",
      " policycount                      | 5.098220813627468    \n",
      " policytermindicator              | 0.11354681866145568  \n",
      " totalbuildinginsurancecoverage   | 1004615.9070116358   \n",
      " totalcontentsinsurancecoverage   | 59525.97385845581    \n",
      " totalinsurancepremiumofthepolicy | 1649.2813681763807   \n",
      "-RECORD 3------------------------------------------------\n",
      " summary                          | min                  \n",
      " basefloodelevation               | -9999.0              \n",
      " basementenclosurecrawlspacetype  | 0                    \n",
      " censustract                      | 1003                 \n",
      " countycode                       | 1001                 \n",
      " crsdiscount                      | 0.0                  \n",
      " elevationdifference              | -9995                \n",
      " federalpolicyfee                 | -1600                \n",
      " hfiaasurcharge                   | -500                 \n",
      " latitude                         | -14.3                \n",
      " longitude                        | -170.7               \n",
      " lowestadjacentgrade              | -9999.9              \n",
      " lowestfloorelevation             | -9997.9              \n",
      " numberoffloorsininsuredbuilding  | 1                    \n",
      " occupancytype                    | 1                    \n",
      " policycost                       | -8731                \n",
      " policycount                      | 1                    \n",
      " policytermindicator              | 1                    \n",
      " totalbuildinginsurancecoverage   | 0                    \n",
      " totalcontentsinsurancecoverage   | 0                    \n",
      " totalinsurancepremiumofthepolicy | -5928                \n",
      "-RECORD 4------------------------------------------------\n",
      " summary                          | max                  \n",
      " basefloodelevation               | 85640.0              \n",
      " basementenclosurecrawlspacetype  | 4                    \n",
      " censustract                      | 78030961200          \n",
      " countycode                       | 78030                \n",
      " crsdiscount                      | 0.45                 \n",
      " elevationdifference              | 9998                 \n",
      " federalpolicyfee                 | 4000                 \n",
      " hfiaasurcharge                   | 2575                 \n",
      " latitude                         | 69.9                 \n",
      " longitude                        | 145.8                \n",
      " lowestadjacentgrade              | 99990.9              \n",
      " lowestfloorelevation             | 99989.0              \n",
      " numberoffloorsininsuredbuilding  | 6                    \n",
      " occupancytype                    | 6                    \n",
      " policycost                       | 1115140              \n",
      " policycount                      | 1203                 \n",
      " policytermindicator              | 9                    \n",
      " totalbuildinginsurancecoverage   | 249750000            \n",
      " totalcontentsinsurancecoverage   | 6000000              \n",
      " totalinsurancepremiumofthepolicy | 1061158              \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "numerical_columns = [col_name for col_name, data_type in df.dtypes if data_type in [\"int\", \"bigint\", \"double\"]]\n",
    "\n",
    "# Select only numerical columns\n",
    "numerical_df = df.select(*numerical_columns)\n",
    "\n",
    "# Generate summary statistics\n",
    "summary_stats = numerical_df.describe()\n",
    "\n",
    "# Show summary statistics\n",
    "summary_stats.show(vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1b200",
   "metadata": {},
   "source": [
    "The summary statistics for the FEMA National Flood Insurance Policy Database provide a comprehensive overview of various policy and property-related numerical attributes. These statistics include measures of central tendency, dispersion, and range, all of which are critical for understanding the distribution and potential data quality issues within the dataset. Below is a detailed analysis of the key statistical summaries:\n",
    "\n",
    "**Central Tendency and Dispersion**\n",
    "1. **Base Flood Elevation:**\n",
    "- Average (Mean): 119.47 ft.\n",
    "- Standard Deviation: 522.49 ft.\n",
    "- Range: -9999 - 85,640 ft.\n",
    "2. **Lowest Adjacent Grade:**\n",
    "- Average (Mean): 129.20 ft.\n",
    "- Standard Deviation: 609.92 ft.\n",
    "- Range: -9,999 - 99,990.9 ft.\n",
    "3. **Lowest Floor Elevation:**\n",
    "- Average (Mean): 385.62 ft.\n",
    "- Standard Deviation: 1,676.42 ft.\n",
    "- Range: -9,997.9 - 99,989 ft.\n",
    "4. **Basement Enclosure Crawl Space Type:**\n",
    "- Average (Mean): 0.37, indicating a slight bias towards lower classifications.\n",
    "- Standard Deviation: 0.86, showing moderate variability within the data.\n",
    "- Range: Min 0 to Max 4, spanning several classification levels.\n",
    "5. **Census Tract:**\n",
    "- Average (Mean): Approximately 2.6 x 10¹⁰.\n",
    "- Standard Deviation: About 1.58 x 10¹⁰, suggesting a wide spread across census tracts.\n",
    "6. **CRS Discount:**\n",
    "- Average: 0.064, typically low across the dataset.\n",
    "- Standard Deviation: 0.091, with most data points close to zero but some higher values.\n",
    "7. **Deductible Amount in Building and Contents Coverage:**\n",
    "- Building Coverage Average: 1.66 with a deviation of 1.46.\n",
    "- Contents Coverage Average: 0.98 with a deviation of 1.05.\n",
    "- Both show low average deductible amounts but with notable variation.\n",
    "8. **Elevation Difference:**\n",
    "- Average: 1.69, indicating minor differences in elevation on average.\n",
    "- Standard Deviation: 3.39, suggesting significant outliers affecting the elevation difference.\n",
    "9. **Policy Related Figures (Policy Cost, Policy Count, Total Insurance Coverage, etc.):**\n",
    "- These values have a high mean and standard deviation, indicating a significant spread in the policy costs and coverages, reflecting diverse insurance policies and property valuations.\n",
    "\n",
    "**Extremes (Minimum and Maximum Values)**\n",
    "- Notable minimums include negative values in **Federal Policy Fee and HFIAA Surcharge**, possibly indicating refunds or adjustments.\n",
    "- The maximum values in **Total Building Insurance Coverage and Total Insurance Premium of the Policy** reach into the hundreds of millions, highlighting cases with exceptionally high insurance coverage.\n",
    "\n",
    "**Implications**\n",
    "The substantial missing data in critical geographical and elevation columns could significantly hinder risk assessment accuracy. The wide variability in policy costs and coverage levels underscores the diverse nature of the insured properties. Accurate and complete data in these fields are crucial for effective risk management and policy pricing in flood insurance.\n",
    "\n",
    "This analysis provides a basis for further data cleaning, particularly in addressing missing values and outliers, which are essential for improving data quality and the reliability of subsequent analyses and decision-making processes based on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c08be-71bc-4c28-823e-d1b2537cc6d6",
   "metadata": {},
   "source": [
    "### Correlations Among Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8de39205-bbba-4da1-8a6a-aa5b67e4855c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------\n",
      " basefloodelevation               | NULL        \n",
      " basementenclosurecrawlspacetype  | 2           \n",
      " censustract                      | 33013038500 \n",
      " countycode                       | 33013       \n",
      " crsdiscount                      | 0.0         \n",
      " elevationdifference              | 999         \n",
      " federalpolicyfee                 | 13          \n",
      " hfiaasurcharge                   | 0           \n",
      " latitude                         | 43.3        \n",
      " longitude                        | -71.8       \n",
      " lowestadjacentgrade              | NULL        \n",
      " lowestfloorelevation             | NULL        \n",
      " numberoffloorsininsuredbuilding  | 2           \n",
      " occupancytype                    | 1           \n",
      " policycost                       | 388         \n",
      " policycount                      | 1           \n",
      " policytermindicator              | 1           \n",
      " totalbuildinginsurancecoverage   | 250000      \n",
      " totalcontentsinsurancecoverage   | 100000      \n",
      " totalinsurancepremiumofthepolicy | 375         \n",
      "-RECORD 1---------------------------------------\n",
      " basefloodelevation               | NULL        \n",
      " basementenclosurecrawlspacetype  | 0           \n",
      " censustract                      | 22063040700 \n",
      " countycode                       | 22063       \n",
      " crsdiscount                      | 0.05        \n",
      " elevationdifference              | 999         \n",
      " federalpolicyfee                 | 35          \n",
      " hfiaasurcharge                   | 0           \n",
      " latitude                         | 30.5        \n",
      " longitude                        | -91.0       \n",
      " lowestadjacentgrade              | NULL        \n",
      " lowestfloorelevation             | NULL        \n",
      " numberoffloorsininsuredbuilding  | 1           \n",
      " occupancytype                    | 3           \n",
      " policycost                       | 315         \n",
      " policycount                      | 1           \n",
      " policytermindicator              | 1           \n",
      " totalbuildinginsurancecoverage   | 16400       \n",
      " totalcontentsinsurancecoverage   | 8800        \n",
      " totalinsurancepremiumofthepolicy | 280         \n",
      "-RECORD 2---------------------------------------\n",
      " basefloodelevation               | NULL        \n",
      " basementenclosurecrawlspacetype  | 0           \n",
      " censustract                      | 45051060204 \n",
      " countycode                       | 45051       \n",
      " crsdiscount                      | 0.0         \n",
      " elevationdifference              | 999         \n",
      " federalpolicyfee                 | 13          \n",
      " hfiaasurcharge                   | 0           \n",
      " latitude                         | 33.7        \n",
      " longitude                        | -79.0       \n",
      " lowestadjacentgrade              | NULL        \n",
      " lowestfloorelevation             | NULL        \n",
      " numberoffloorsininsuredbuilding  | 1           \n",
      " occupancytype                    | 1           \n",
      " policycost                       | 348         \n",
      " policycount                      | 1           \n",
      " policytermindicator              | 1           \n",
      " totalbuildinginsurancecoverage   | 250000      \n",
      " totalcontentsinsurancecoverage   | 100000      \n",
      " totalinsurancepremiumofthepolicy | 335         \n",
      "-RECORD 3---------------------------------------\n",
      " basefloodelevation               | 519.0       \n",
      " basementenclosurecrawlspacetype  | 2           \n",
      " censustract                      | 1055001200  \n",
      " countycode                       | 1055        \n",
      " crsdiscount                      | 0.0         \n",
      " elevationdifference              | -2          \n",
      " federalpolicyfee                 | 35          \n",
      " hfiaasurcharge                   | 0           \n",
      " latitude                         | 34.0        \n",
      " longitude                        | -86.0       \n",
      " lowestadjacentgrade              | 517.3       \n",
      " lowestfloorelevation             | 517.3       \n",
      " numberoffloorsininsuredbuilding  | 3           \n",
      " occupancytype                    | 1           \n",
      " policycost                       | 951         \n",
      " policycount                      | 1           \n",
      " policytermindicator              | 1           \n",
      " totalbuildinginsurancecoverage   | 174900      \n",
      " totalcontentsinsurancecoverage   | 21000       \n",
      " totalinsurancepremiumofthepolicy | 916         \n",
      "-RECORD 4---------------------------------------\n",
      " basefloodelevation               | 7.0         \n",
      " basementenclosurecrawlspacetype  | 0           \n",
      " censustract                      | 12086000115 \n",
      " countycode                       | 12086       \n",
      " crsdiscount                      | 0.0         \n",
      " elevationdifference              | 0           \n",
      " federalpolicyfee                 | 35          \n",
      " hfiaasurcharge                   | 0           \n",
      " latitude                         | 26.0        \n",
      " longitude                        | -80.1       \n",
      " lowestadjacentgrade              | NULL        \n",
      " lowestfloorelevation             | 6.9         \n",
      " numberoffloorsininsuredbuilding  | 1           \n",
      " occupancytype                    | 1           \n",
      " policycost                       | 1323        \n",
      " policycount                      | 1           \n",
      " policytermindicator              | 1           \n",
      " totalbuildinginsurancecoverage   | 250000      \n",
      " totalcontentsinsurancecoverage   | 100000      \n",
      " totalinsurancepremiumofthepolicy | 1288        \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numerical_df.show(5,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "815d99a1-7348-4b2c-a42a-1aee2231685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_col = \"features\"\n",
    "input = numerical_df.columns\n",
    "output = [v_col + str(i) for i in range(len(input))]\n",
    "imputer = Imputer(strategy='mean',inputCols=input,outputCols=output)\n",
    "changed_df = imputer.fit(numerical_df).transform(numerical_df) \n",
    "\n",
    "assembler = VectorAssembler(inputCols=output,outputCol=v_col)\n",
    "numerical_df_vector = assembler.transform(changed_df).select(v_col)\n",
    "numerical_df_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a84814-8007-4d66-9ff1-a65c567c8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = Correlation.corr(numerical_df_vector,v_col).collect()[0][0]\n",
    "corr_matrix = matrix.toArray().tolist()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7fb26-e714-4477-8744-0ee9f7099bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc495c-9b0c-4c2a-862a-e3911976008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['basefloodelevation',\n",
    " 'basementenclosurecrawlspacetype',\n",
    " 'censustract',\n",
    " 'countycode',\n",
    " 'crsdiscount',\n",
    " 'elevationdifference',\n",
    " 'federalpolicyfee',\n",
    " 'hfiaasurcharge',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'lowestadjacentgrade',\n",
    " 'lowestfloorelevation',\n",
    " 'numberoffloorsininsuredbuilding',\n",
    " 'occupancytype',\n",
    " 'policycost',\n",
    " 'policycount',\n",
    " 'policytermindicator',\n",
    " 'totalbuildinginsurancecoverage',\n",
    " 'totalcontentsinsurancecoverage',\n",
    " 'totalinsurancepremiumofthepolicy']\n",
    "df_c = spark.createDataFrame(corr_matrix,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dd59ba2-1e7e-48af-aab5-ce12e63ebbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 11:05:45 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 715)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "24/05/28 11:05:45 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 715) (192.168.4.39 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "24/05/28 11:05:45 ERROR TaskSetManager: Task 0 in stage 26.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o444.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 715) (192.168.4.39 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbasefloodelevation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbasementenclosurecrawlspacetype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcensustract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o444.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 715) (192.168.4.39 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_c.select('basefloodelevation',\n",
    " 'basementenclosurecrawlspacetype',\n",
    " 'censustract').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0310dcee-68c8-452a-a83f-20273013c028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 11:05:48 ERROR Executor: Exception in task 0.0 in stage 27.0 (TID 716)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "24/05/28 11:05:48 WARN TaskSetManager: Lost task 0.0 in stage 27.0 (TID 716) (192.168.4.39 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\n",
      "Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "24/05/28 11:05:48 ERROR TaskSetManager: Task 0 in stage 27.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o481.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 716) (192.168.4.39 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcountycode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcrsdiscount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43melevationdifference\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o481.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 716) (192.168.4.39 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/mengkong/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main\n    raise PySparkRuntimeError(\npyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 11) than that in driver 3.9, PySpark cannot run with different minor versions.\nPlease check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_c.select('countycode',\n",
    " 'crsdiscount',\n",
    " 'elevationdifference').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca97081-e7db-4b49-b4ef-63ebd3cf3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.select('federalpolicyfee',\n",
    " 'hfiaasurcharge',\n",
    " 'latitude').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0cad91-59a1-47e7-a55e-4bac354bca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.select('longitude',\n",
    " 'lowestadjacentgrade',\n",
    " 'lowestfloorelevation').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852d6f6-8a18-46c7-bbdf-d9444f9fe7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.select('numberoffloorsininsuredbuilding',\n",
    " 'occupancytype',\n",
    " 'policycost').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8bef1-6303-4d11-9636-0b03ea59ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.select('policycount',\n",
    " 'policytermindicator',\n",
    " 'totalbuildinginsurancecoverage').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add942fc-df40-4bfd-b481-ccc8d8a561c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.select('totalcontentsinsurancecoverage',\n",
    " 'totalinsurancepremiumofthepolicy').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c070f80-06aa-4100-ad23-ec46429cfd59",
   "metadata": {},
   "source": [
    "The dataframe new_df is the dataframe with the imputed mean values and this code checks that there are zero missing values and the column names are changed from feature0, feature1 and so on to the original column names within the new_df dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7da6c40c-952f-4142-bb58-ef0d6ce813c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------\n",
      " features0  | 119.46875184553842 \n",
      " features1  | 2                  \n",
      " features2  | 33013038500        \n",
      " features3  | 33013              \n",
      " features4  | 0.0                \n",
      " features5  | 999                \n",
      " features6  | 13                 \n",
      " features7  | 0                  \n",
      " features8  | 43.3               \n",
      " features9  | -71.8              \n",
      " features10 | 129.1970600006573  \n",
      " features11 | 385.6220114835789  \n",
      " features12 | 2                  \n",
      " features13 | 1                  \n",
      " features14 | 388                \n",
      " features15 | 1                  \n",
      " features16 | 1                  \n",
      " features17 | 250000             \n",
      " features18 | 100000             \n",
      " features19 | 375                \n",
      "-RECORD 1------------------------\n",
      " features0  | 119.46875184553842 \n",
      " features1  | 0                  \n",
      " features2  | 22063040700        \n",
      " features3  | 22063              \n",
      " features4  | 0.05               \n",
      " features5  | 999                \n",
      " features6  | 35                 \n",
      " features7  | 0                  \n",
      " features8  | 30.5               \n",
      " features9  | -91.0              \n",
      " features10 | 129.1970600006573  \n",
      " features11 | 385.6220114835789  \n",
      " features12 | 1                  \n",
      " features13 | 3                  \n",
      " features14 | 315                \n",
      " features15 | 1                  \n",
      " features16 | 1                  \n",
      " features17 | 16400              \n",
      " features18 | 8800               \n",
      " features19 | 280                \n",
      "-RECORD 2------------------------\n",
      " features0  | 119.46875184553842 \n",
      " features1  | 0                  \n",
      " features2  | 45051060204        \n",
      " features3  | 45051              \n",
      " features4  | 0.0                \n",
      " features5  | 999                \n",
      " features6  | 13                 \n",
      " features7  | 0                  \n",
      " features8  | 33.7               \n",
      " features9  | -79.0              \n",
      " features10 | 129.1970600006573  \n",
      " features11 | 385.6220114835789  \n",
      " features12 | 1                  \n",
      " features13 | 1                  \n",
      " features14 | 348                \n",
      " features15 | 1                  \n",
      " features16 | 1                  \n",
      " features17 | 250000             \n",
      " features18 | 100000             \n",
      " features19 | 335                \n",
      "-RECORD 3------------------------\n",
      " features0  | 519.0              \n",
      " features1  | 2                  \n",
      " features2  | 1055001200         \n",
      " features3  | 1055               \n",
      " features4  | 0.0                \n",
      " features5  | -2                 \n",
      " features6  | 35                 \n",
      " features7  | 0                  \n",
      " features8  | 34.0               \n",
      " features9  | -86.0              \n",
      " features10 | 517.3              \n",
      " features11 | 517.3              \n",
      " features12 | 3                  \n",
      " features13 | 1                  \n",
      " features14 | 951                \n",
      " features15 | 1                  \n",
      " features16 | 1                  \n",
      " features17 | 174900             \n",
      " features18 | 21000              \n",
      " features19 | 916                \n",
      "-RECORD 4------------------------\n",
      " features0  | 7.0                \n",
      " features1  | 0                  \n",
      " features2  | 12086000115        \n",
      " features3  | 12086              \n",
      " features4  | 0.0                \n",
      " features5  | 0                  \n",
      " features6  | 35                 \n",
      " features7  | 0                  \n",
      " features8  | 26.0               \n",
      " features9  | -80.1              \n",
      " features10 | 129.1970600006573  \n",
      " features11 | 6.9                \n",
      " features12 | 1                  \n",
      " features13 | 1                  \n",
      " features14 | 1323               \n",
      " features15 | 1                  \n",
      " features16 | 1                  \n",
      " features17 | 250000             \n",
      " features18 | 100000             \n",
      " features19 | 1288               \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = changed_df.drop('basefloodelevation',\n",
    " 'basementenclosurecrawlspacetype',\n",
    " 'censustract',\n",
    " 'countycode',\n",
    " 'crsdiscount',\n",
    " 'elevationdifference',\n",
    " 'federalpolicyfee',\n",
    " 'hfiaasurcharge',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'lowestadjacentgrade',\n",
    " 'lowestfloorelevation',\n",
    " 'numberoffloorsininsuredbuilding',\n",
    " 'occupancytype',\n",
    " 'policycost',\n",
    " 'policycount',\n",
    " 'policytermindicator',\n",
    " 'totalbuildinginsurancecoverage',\n",
    " 'totalcontentsinsurancecoverage',\n",
    " 'totalinsurancepremiumofthepolicy')\n",
    "new_df.show(5,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32070b59-92cf-4db7-a782-65dd7bab243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_missing_vals = new_df.select(*(spark_sum(col(i).isNull().cast(\"int\")).alias(i) for i in new_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91818d10-cebf-454c-a0f1-a0a3bcbb3b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 149:======================================================>(96 + 1) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " features0  | 0   \n",
      " features1  | 0   \n",
      " features2  | 0   \n",
      " features3  | 0   \n",
      " features4  | 0   \n",
      " features5  | 0   \n",
      " features6  | 0   \n",
      " features7  | 0   \n",
      " features8  | 0   \n",
      " features9  | 0   \n",
      " features10 | 0   \n",
      " features11 | 0   \n",
      " features12 | 0   \n",
      " features13 | 0   \n",
      " features14 | 0   \n",
      " features15 | 0   \n",
      " features16 | 0   \n",
      " features17 | 0   \n",
      " features18 | 0   \n",
      " features19 | 0   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "new_df_missing_vals.show(vertical= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5967b8f8-115b-4026-998f-302c1719de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------\n",
      " basefloodelevation               | 119.46875184553842 \n",
      " basementenclosurecrawlspacetype  | 2                  \n",
      " censustract                      | 33013038500        \n",
      " countycode                       | 33013              \n",
      " crsdiscount                      | 0.0                \n",
      " elevationdifference              | 999                \n",
      " federalpolicyfee                 | 13                 \n",
      " hfiaasurcharge                   | 0                  \n",
      " latitude                         | 43.3               \n",
      " longitude                        | -71.8              \n",
      " lowestadjacentgrade              | 129.1970600006573  \n",
      " lowestfloorelevation             | 385.6220114835789  \n",
      " numberoffloorsininsuredbuilding  | 2                  \n",
      " occupancytype                    | 1                  \n",
      " policycost                       | 388                \n",
      " policycount                      | 1                  \n",
      " policytermindicator              | 1                  \n",
      " totalbuildinginsurancecoverage   | 250000             \n",
      " totalcontentsinsurancecoverage   | 100000             \n",
      " totalinsurancepremiumofthepolicy | 375                \n",
      "-RECORD 1----------------------------------------------\n",
      " basefloodelevation               | 119.46875184553842 \n",
      " basementenclosurecrawlspacetype  | 0                  \n",
      " censustract                      | 22063040700        \n",
      " countycode                       | 22063              \n",
      " crsdiscount                      | 0.05               \n",
      " elevationdifference              | 999                \n",
      " federalpolicyfee                 | 35                 \n",
      " hfiaasurcharge                   | 0                  \n",
      " latitude                         | 30.5               \n",
      " longitude                        | -91.0              \n",
      " lowestadjacentgrade              | 129.1970600006573  \n",
      " lowestfloorelevation             | 385.6220114835789  \n",
      " numberoffloorsininsuredbuilding  | 1                  \n",
      " occupancytype                    | 3                  \n",
      " policycost                       | 315                \n",
      " policycount                      | 1                  \n",
      " policytermindicator              | 1                  \n",
      " totalbuildinginsurancecoverage   | 16400              \n",
      " totalcontentsinsurancecoverage   | 8800               \n",
      " totalinsurancepremiumofthepolicy | 280                \n",
      "-RECORD 2----------------------------------------------\n",
      " basefloodelevation               | 119.46875184553842 \n",
      " basementenclosurecrawlspacetype  | 0                  \n",
      " censustract                      | 45051060204        \n",
      " countycode                       | 45051              \n",
      " crsdiscount                      | 0.0                \n",
      " elevationdifference              | 999                \n",
      " federalpolicyfee                 | 13                 \n",
      " hfiaasurcharge                   | 0                  \n",
      " latitude                         | 33.7               \n",
      " longitude                        | -79.0              \n",
      " lowestadjacentgrade              | 129.1970600006573  \n",
      " lowestfloorelevation             | 385.6220114835789  \n",
      " numberoffloorsininsuredbuilding  | 1                  \n",
      " occupancytype                    | 1                  \n",
      " policycost                       | 348                \n",
      " policycount                      | 1                  \n",
      " policytermindicator              | 1                  \n",
      " totalbuildinginsurancecoverage   | 250000             \n",
      " totalcontentsinsurancecoverage   | 100000             \n",
      " totalinsurancepremiumofthepolicy | 335                \n",
      "-RECORD 3----------------------------------------------\n",
      " basefloodelevation               | 519.0              \n",
      " basementenclosurecrawlspacetype  | 2                  \n",
      " censustract                      | 1055001200         \n",
      " countycode                       | 1055               \n",
      " crsdiscount                      | 0.0                \n",
      " elevationdifference              | -2                 \n",
      " federalpolicyfee                 | 35                 \n",
      " hfiaasurcharge                   | 0                  \n",
      " latitude                         | 34.0               \n",
      " longitude                        | -86.0              \n",
      " lowestadjacentgrade              | 517.3              \n",
      " lowestfloorelevation             | 517.3              \n",
      " numberoffloorsininsuredbuilding  | 3                  \n",
      " occupancytype                    | 1                  \n",
      " policycost                       | 951                \n",
      " policycount                      | 1                  \n",
      " policytermindicator              | 1                  \n",
      " totalbuildinginsurancecoverage   | 174900             \n",
      " totalcontentsinsurancecoverage   | 21000              \n",
      " totalinsurancepremiumofthepolicy | 916                \n",
      "-RECORD 4----------------------------------------------\n",
      " basefloodelevation               | 7.0                \n",
      " basementenclosurecrawlspacetype  | 0                  \n",
      " censustract                      | 12086000115        \n",
      " countycode                       | 12086              \n",
      " crsdiscount                      | 0.0                \n",
      " elevationdifference              | 0                  \n",
      " federalpolicyfee                 | 35                 \n",
      " hfiaasurcharge                   | 0                  \n",
      " latitude                         | 26.0               \n",
      " longitude                        | -80.1              \n",
      " lowestadjacentgrade              | 129.1970600006573  \n",
      " lowestfloorelevation             | 6.9                \n",
      " numberoffloorsininsuredbuilding  | 1                  \n",
      " occupancytype                    | 1                  \n",
      " policycost                       | 1323               \n",
      " policycount                      | 1                  \n",
      " policytermindicator              | 1                  \n",
      " totalbuildinginsurancecoverage   | 250000             \n",
      " totalcontentsinsurancecoverage   | 100000             \n",
      " totalinsurancepremiumofthepolicy | 1288               \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_names = ['basefloodelevation',\n",
    " 'basementenclosurecrawlspacetype',\n",
    " 'censustract',\n",
    " 'countycode',\n",
    " 'crsdiscount',\n",
    " 'elevationdifference',\n",
    " 'federalpolicyfee',\n",
    " 'hfiaasurcharge',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'lowestadjacentgrade',\n",
    " 'lowestfloorelevation',\n",
    " 'numberoffloorsininsuredbuilding',\n",
    " 'occupancytype',\n",
    " 'policycost',\n",
    " 'policycount',\n",
    " 'policytermindicator',\n",
    " 'totalbuildinginsurancecoverage',\n",
    " 'totalcontentsinsurancecoverage',\n",
    " 'totalinsurancepremiumofthepolicy']\n",
    "new_df = new_df.toDF(*column_names)\n",
    "new_df.show(5,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc91cdc1-2ab9-433a-bc72-dc2243b32a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('basefloodelevation', 'double'), ('basementenclosurecrawlspacetype', 'int'), ('censustract', 'bigint'), ('countycode', 'int'), ('crsdiscount', 'double'), ('elevationdifference', 'int'), ('federalpolicyfee', 'int'), ('hfiaasurcharge', 'int'), ('latitude', 'double'), ('longitude', 'double'), ('lowestadjacentgrade', 'double'), ('lowestfloorelevation', 'double'), ('numberoffloorsininsuredbuilding', 'int'), ('occupancytype', 'int'), ('policycost', 'int'), ('policycount', 'int'), ('policytermindicator', 'int'), ('totalbuildinginsurancecoverage', 'int'), ('totalcontentsinsurancecoverage', 'int'), ('totalinsurancepremiumofthepolicy', 'int')]\n"
     ]
    }
   ],
   "source": [
    "print(new_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1edba34-6010-47f6-83be-5df33ed90094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Milestone 3: Construct Linear Regression model to perform Feature Importance (find weights of features, choose top weights' features to keep)\n",
    "# creating train and test sets\n",
    "# X = new_df.drop('totalinsurancepremiumofthepolicy')\n",
    "# y = df['totalinsurancepremiumofthepolicy']\n",
    "# ASK: Is 'features' correct?\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['basefloodelevation',\n",
    " 'basementenclosurecrawlspacetype',\n",
    " 'censustract',\n",
    " 'countycode',\n",
    " 'crsdiscount',\n",
    " 'elevationdifference',\n",
    " 'federalpolicyfee',\n",
    " 'hfiaasurcharge',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'lowestadjacentgrade',\n",
    " 'lowestfloorelevation',\n",
    " 'numberoffloorsininsuredbuilding',\n",
    " 'occupancytype',\n",
    " 'policycost',\n",
    " 'policycount',\n",
    " 'policytermindicator',\n",
    " 'totalbuildinginsurancecoverage',\n",
    " 'totalcontentsinsurancecoverage'],\n",
    "    outputCol = 'features')\n",
    "new_df = assembler.transform(new_df)\n",
    "final_data = new_df.select('features', 'totalinsurancepremiumofthepolicy')\n",
    "\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium', regParam = 0.3)\n",
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a593fcd9-da7e-4e1b-98c1-b78588ca6429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.0003436622598304444,-2.7028946217444103,-3.1266371101423445e-10,0.00020196885521765493,-28.57518121293229,-0.0035443198285836615,-1.148271437106802,-1.408019904138137,0.27984272034633223,0.05025740009216324,-0.0010169937528697862,-0.00041071919113279414,0.9157052648296696,3.9822580516660384,0.9342110367211139,3.96760704505804,7.069634236467563,-1.4447964502428905e-05,-1.343380126652838e-05]\n"
     ]
    }
   ],
   "source": [
    "# Milestone 3: Print coefficients of the features (will run this cell AFTER LinReg is successful)\n",
    "coefficients = lr_model.coefficients\n",
    "print(\"Coefficients: {}\".format(lr_model.coefficients)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47538ebb-f710-4749-8f53-27ef6999ed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "  crsdiscount: 28.575\n",
      "  policytermindicator: 7.070\n",
      "  occupancytype: 3.982\n",
      "  policycount: 3.968\n",
      "  basementenclosurecrawlspacetype: 2.703\n",
      "  hfiaasurcharge: 1.408\n",
      "  federalpolicyfee: 1.148\n",
      "  policycost: 0.934\n",
      "  numberoffloorsininsuredbuilding: 0.916\n",
      "  latitude: 0.280\n",
      "  longitude: 0.050\n",
      "  elevationdifference: 0.004\n",
      "  lowestadjacentgrade: 0.001\n",
      "  lowestfloorelevation: 0.000\n",
      "  basefloodelevation: 0.000\n",
      "  countycode: 0.000\n",
      "  totalbuildinginsurancecoverage: 0.000\n",
      "  totalcontentsinsurancecoverage: 0.000\n",
      "  censustract: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Milestone 3: Construct Feature Importance to find top 10 features to choose to represent our dataset.\n",
    "feature_importance = sorted(list(zip(new_df.columns[:-1], map(abs, coefficients))), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "for feature, importance in feature_importance:\n",
    "    print(\"  {}: {:.3f}\".format(feature, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b264f4b-fe62-4a73-9677-f760ec3fcd8c",
   "metadata": {},
   "source": [
    "# Construct 5 Linear Regression Models to find Ideal Range for Model Complexity\n",
    "#### Using the Feature Importance model information, will start by training a Linear Regression Model with 1 feature (Top Feature Importance), then 5 features, 10 features, 15 features, and finally 19 features. There are a total 20 numerical features in the FEMA dataset, and the totalinsurancepremiumofthepolicy feature is the dependent variable. \n",
    "\n",
    "### Linear Regression Model with 1 Feature: crsdiscount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5551d2d-146e-4d18-a789-115708fb22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler_f1 = VectorAssembler(\n",
    "    inputCols=['crsdiscount'], outputCol = 'predict_features_1')\n",
    "\n",
    "new_df = assembler_f1.transform(new_df)\n",
    "\n",
    "df_f1 = new_df.select('predict_features_1', 'totalinsurancepremiumofthepolicy')\n",
    "train_data_f1, test_data_f1 = df_f1.randomSplit([0.7, 0.3], seed = 42)\n",
    "\n",
    "lr_f1 = LinearRegression(featuresCol = 'predict_features_1', labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f1', regParam = 0.3)\n",
    "lr_model_f1 = lr_f1.fit(train_data_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12ac0cb3-e0eb-469c-854d-cd9b90b7346b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:====================================================>  (93 + 4) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data for Linear Regression using 1 feature: 1643.9152835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 161:======================================================>(96 + 1) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RMSE for Train data\n",
    "predictions_rmse_train_f1 = lr_model_f1.transform(train_data_f1)\n",
    "\n",
    "evaluator_f1 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f1', metricName = 'rmse')\n",
    "rmse_train_f1 = evaluator_f1.evaluate(predictions_rmse_train_f1)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data for Linear Regression using 1 feature: {:.7f}\".format(rmse_train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5701eac2-346c-451c-b91d-0d22b82a1fdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 163:====================================================>  (92 + 5) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on train data for Linear Regression using 1 feature: 0.0034574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 163:=====================================================> (94 + 3) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# R-squared for Train data\n",
    "evaluator_r2_f1 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f1', metricName = 'r2')\n",
    "r2_train_f1 = evaluator_r2_f1.evaluate(predictions_rmse_train_f1)\n",
    "print(\"R-squared (R2) on train data for Linear Regression using 1 feature: {:.7f}\".format(r2_train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5efe0cc9-7dbd-4e28-bebf-e3573dcc684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 165:===================================================>   (91 + 6) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Linear Regression using 1 feature: 1652.3239030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 165:====================================================>  (93 + 4) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RMSE for Test data\n",
    "predictions_rmse_test_f1 = lr_model_f1.transform(test_data_f1)\n",
    "\n",
    "evaluator_f1 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f1', metricName = 'rmse')\n",
    "rmse_test_f1 = evaluator_f1.evaluate(predictions_rmse_test_f1)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data for Linear Regression using 1 feature: {:.7f}\".format(rmse_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2b5519d-c7eb-4ffe-8f30-c3e09802e850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:====================================================>  (93 + 4) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data for Linear Regression using 1 feature: 0.0033947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 167:=====================================================> (94 + 3) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# R-squared for Test data\n",
    "evaluator_r2_f1 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f1', metricName = 'r2')\n",
    "r2_test_f1 = evaluator_r2_f1.evaluate(predictions_rmse_test_f1)\n",
    "print(\"R-squared (R2) on test data for Linear Regression using 1 feature: {:.7f}\".format(r2_test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08f857-c94c-47a9-8810-2e1f8579ca49",
   "metadata": {},
   "source": [
    "### Linear Regression Model with 5 Features: \n",
    "#### crsdiscount', 'policytermindicator', 'occupancytype', 'policycount', basementenclosurecrawlspacetype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a905b89f-5c7b-46cb-9d55-dcfeccd49ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler_f5 = VectorAssembler(\n",
    "    inputCols=['crsdiscount',\n",
    "  'policytermindicator',\n",
    "  'occupancytype',\n",
    "  'policycount',\n",
    "  'basementenclosurecrawlspacetype'], outputCol = 'predict_features_5')\n",
    "\n",
    "new_df = assembler_f5.transform(new_df)\n",
    "\n",
    "df_f5 = new_df.select('predict_features_5', 'totalinsurancepremiumofthepolicy')\n",
    "train_data_f5, test_data_f5 = df_f5.randomSplit([0.7, 0.3], seed = 42)\n",
    "\n",
    "lr_f5 = LinearRegression(featuresCol = 'predict_features_5', labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f5', regParam = 0.3)\n",
    "lr_model_f5 = lr_f5.fit(train_data_f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1af9377b-792c-4da2-b5f4-84bf9ed49a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 173:====================================================>  (92 + 5) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data for Linear Regression using 5 features: 1407.3976759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 173:=====================================================> (95 + 2) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RMSE for Train data\n",
    "predictions_rmse_train_f5 = lr_model_f5.transform(train_data_f5)\n",
    "\n",
    "evaluator_f5 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f5', metricName = 'rmse')\n",
    "rmse_train_f5 = evaluator_f5.evaluate(predictions_rmse_train_f5)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data for Linear Regression using 5 features: {:.7f}\".format(rmse_train_f5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "724aae0e-2edb-402a-a1db-1ce5e221d3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:====================================================>  (92 + 5) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on train data for Linear Regression using 5 features: 0.2814601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 175:=====================================================> (94 + 3) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# R-squared for Train data\n",
    "evaluator_r2_f5 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f5', metricName = 'r2')\n",
    "r2_train_f5 = evaluator_r2_f5.evaluate(predictions_rmse_train_f5)\n",
    "print(\"R-squared (R2) on train data for Linear Regression using 5 features: {:.7f}\".format(r2_train_f5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2b7e997-9d6c-419a-b05f-4a987d42a2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 177:===================================================>   (91 + 6) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Linear Regression using 5 features: 1375.8439908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 177:====================================================>  (93 + 4) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RMSE for Test data\n",
    "predictions_rmse_test_f5 = lr_model_f5.transform(test_data_f5)\n",
    "\n",
    "evaluator_f5 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f5', metricName = 'rmse')\n",
    "rmse_test_f5 = evaluator_f5.evaluate(predictions_rmse_test_f5)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data for Linear Regression using 5 features: {:.7f}\".format(rmse_test_f5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cead9b56-8b4f-4526-a5d4-03e3696306d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 179:====================================================>  (92 + 5) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data for Linear Regression using 5 features: 0.2815993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# R-squared for Test data\n",
    "evaluator_r2_f5 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f5', metricName = 'r2')\n",
    "r2_test_f5 = evaluator_r2_f5.evaluate(predictions_rmse_test_f5)\n",
    "print(\"R-squared (R2) on test data for Linear Regression using 5 features: {:.7f}\".format(r2_test_f5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a35d3-c236-4ffb-a64d-7ffe9993b640",
   "metadata": {},
   "source": [
    "### Linear Regression Model with 10 Features:\n",
    "#### 'basementenclosurecrawlspacetype', 'crsdiscount', 'federalpolicyfee', 'hfiaasurcharge', 'latitude', 'numberoffloorsininsuredbuilding', 'occupancytype', 'policycost', 'policycount', 'policytermindicator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e3436f1-b2e9-430b-b668-95eea5508254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Milestone 3: Construct Linear Regression prediction model with the new Chosen 10 Features\n",
    "# f10 = LinReg model with top 10 features (chosen from Feature Importance)\n",
    "\n",
    "assembler_f10 = VectorAssembler(\n",
    "    inputCols=['basementenclosurecrawlspacetype',\n",
    " 'crsdiscount',\n",
    " 'federalpolicyfee',\n",
    " 'hfiaasurcharge',\n",
    " 'latitude',\n",
    " 'numberoffloorsininsuredbuilding',\n",
    " 'occupancytype',\n",
    " 'policycost',\n",
    " 'policycount',\n",
    " 'policytermindicator'],\n",
    "    outputCol = 'predict_features_10')\n",
    "\n",
    "new_df = assembler_f10.transform(new_df)\n",
    "\n",
    "df_f10 = new_df.select('predict_features_10', 'totalinsurancepremiumofthepolicy')\n",
    "train_data_f10, test_data_f10 = df_f10.randomSplit([0.7, 0.3], seed = 42)\n",
    "\n",
    "lr_f10 = LinearRegression(featuresCol = 'predict_features_10', labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f10', regParam = 0.3)\n",
    "lr_model_f10 = lr_f10.fit(train_data_f10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5bab97a-19b3-424d-9a22-04580ceb2112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 185:====================================================>  (92 + 5) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data for Linear Regression using 10 features: 118.1325824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 185:=====================================================> (94 + 3) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Milestone 3: RMSE for Train data\n",
    "# RMSE ranges from 0 to infinity, lower RMSE the better, (higher weight to larger errors)\n",
    "predictions_rmse_train_f10 = lr_model_f10.transform(train_data_f10)\n",
    "\n",
    "evaluator_f10 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f10', metricName = 'rmse')\n",
    "rmse_train_f10 = evaluator_f10.evaluate(predictions_rmse_train_f10)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data for Linear Regression using 10 features: {:.7f}\".format(rmse_train_f10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c919290e-ad2a-4fa4-aa6f-9903ce0a8531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 187:====================================================>  (92 + 5) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on train data for Linear Regression using 10 features: 0.9949013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 187:=====================================================> (95 + 2) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# R-squared for Train data\n",
    "evaluator_r2_f10 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f10', metricName = 'r2')\n",
    "r2_train_f10 = evaluator_r2_f10.evaluate(predictions_rmse_train_f10)\n",
    "print(\"R-squared (R2) on train data for Linear Regression using 10 features: {:.7f}\".format(r2_train_f10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e51187e9-29b6-4ff8-b881-30118c37a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 189:=====================================================> (94 + 3) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Linear Regression using 10 features: 117.1184727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 189:=====================================================> (95 + 2) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RMSE for Test data\n",
    "predictions_rmse_test_f10 = lr_model_f10.transform(test_data_f10)\n",
    "\n",
    "evaluator_f10 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f10', metricName = 'rmse')\n",
    "rmse_test_f10 = evaluator_f10.evaluate(predictions_rmse_test_f10)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data for Linear Regression using 10 features: {:.7f}\".format(rmse_test_f10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "937352cf-d6ee-4e64-aef7-d8aa4abb68ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 191:====================================================>  (93 + 4) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data for Linear Regression using 10 features: 0.9948833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# R-squared for Test data\n",
    "evaluator_r2_f10 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f10', metricName = 'r2')\n",
    "r2_test_f10 = evaluator_r2_f10.evaluate(predictions_rmse_test_f10)\n",
    "print(\"R-squared (R2) on test data for Linear Regression using 10 features: {:.7f}\".format(r2_test_f10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff452090-d2c7-47a5-9d6b-c291bdce2303",
   "metadata": {},
   "source": [
    "### Linear Regression Model with 15 Features\n",
    "#### 'crsdiscount', 'policytermindicator', 'occupancytype', 'policycount', 'basementenclosurecrawlspacetype', 'hfiaasurcharge', 'federalpolicyfee', 'policycost', 'numberoffloorsininsuredbuilding', 'latitude', 'longitude', 'elevationdifference', 'lowestadjacentgrade', 'lowestfloorelevation', 'basefloodelevation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8f344e6-94c7-409e-aaab-48263bacbe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler_f15 = VectorAssembler(\n",
    "    inputCols=['crsdiscount',\n",
    "  'policytermindicator',\n",
    "  'occupancytype',\n",
    "  'policycount',\n",
    "  'basementenclosurecrawlspacetype',\n",
    "  'hfiaasurcharge',\n",
    "  'federalpolicyfee',\n",
    "  'policycost',\n",
    "  'numberoffloorsininsuredbuilding',\n",
    "  'latitude',\n",
    "  'longitude',\n",
    "  'elevationdifference',\n",
    "  'lowestadjacentgrade',\n",
    "  'lowestfloorelevation',\n",
    "  'basefloodelevation'], outputCol = 'predict_features_15')\n",
    "\n",
    "new_df = assembler_f15.transform(new_df)\n",
    "\n",
    "df_f15 = new_df.select('predict_features_15', 'totalinsurancepremiumofthepolicy')\n",
    "train_data_f15, test_data_f15 = df_f15.randomSplit([0.7, 0.3], seed = 42)\n",
    "\n",
    "lr_f15 = LinearRegression(featuresCol = 'predict_features_15', labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f15', regParam = 0.3)\n",
    "lr_model_f15 = lr_f15.fit(train_data_f15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "624e08ef-971b-46a6-ae84-07a2778f0a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 197:=====================================================> (94 + 3) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data for Linear Regression using 15 features: 117.6225831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 197:=====================================================> (95 + 2) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RMSE for Train data\n",
    "predictions_rmse_train_f15 = lr_model_f15.transform(train_data_f15)\n",
    "\n",
    "evaluator_f15 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f15', metricName = 'rmse')\n",
    "rmse_train_f15 = evaluator_f15.evaluate(predictions_rmse_train_f15)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data for Linear Regression using 15 features: {:.7f}\".format(rmse_train_f15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91a3556b-8a48-4320-b816-4fc47968f70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:====================================================>  (93 + 4) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on train data for Linear Regression using 15 features: 0.9948743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 199:======================================================>(96 + 1) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# R-squared for Train data\n",
    "evaluator_r2_f15 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f15', metricName = 'r2')\n",
    "r2_train_f15 = evaluator_r2_f15.evaluate(predictions_rmse_train_f15)\n",
    "print(\"R-squared (R2) on train data for Linear Regression using 15 features: {:.7f}\".format(r2_train_f15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "acc61ffb-6b44-417b-85d4-e966aadfc43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 201:====================================================>  (93 + 4) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Linear Regression using 15 features: 118.2675894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 201:=====================================================> (95 + 2) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RMSE for Test data\n",
    "predictions_rmse_test_f15 = lr_model_f15.transform(test_data_f15)\n",
    "\n",
    "evaluator_f15 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f15', metricName = 'rmse')\n",
    "rmse_test_f15 = evaluator_f15.evaluate(predictions_rmse_test_f15)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data for Linear Regression using 15 features: {:.7f}\".format(rmse_test_f15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17eead96-f5ca-4322-bb55-01340066780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 203:====================================================>  (92 + 5) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data for Linear Regression using 15 features: 0.9949487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 203:====================================================>  (93 + 4) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# R-squared for Test data\n",
    "evaluator_r2_f15 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f15', metricName = 'r2')\n",
    "r2_test_f15 = evaluator_r2_f15.evaluate(predictions_rmse_test_f15)\n",
    "print(\"R-squared (R2) on test data for Linear Regression using 15 features: {:.7f}\".format(r2_test_f15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73f50d-88a3-4705-9c3d-5eb18e28c744",
   "metadata": {},
   "source": [
    "### Linear Regression Model with 19 Features\n",
    "#### 'crsdiscount', 'policytermindicator', 'occupancytype', 'policycount', 'basementenclosurecrawlspacetype', 'hfiaasurcharge', 'federalpolicyfee', 'policycost', 'numberoffloorsininsuredbuilding', 'latitude', 'longitude', 'elevationdifference', 'lowestadjacentgrade:', 'lowestfloorelevation', 'basefloodelevation', 'countycode', 'totalbuildinginsurancecoverage', 'totalcontentsinsurancecoverage', 'censustract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b7ae311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check if 'predict_features_19' already exists, if so, drop it\n",
    "if 'predict_features_19' in new_df.columns:\n",
    "    new_df = new_df.drop('predict_features_19')\n",
    "\n",
    "# Apply VectorAssembler\n",
    "assembler_f19 = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'crsdiscount', 'policytermindicator', 'occupancytype', 'policycount',\n",
    "        'basementenclosurecrawlspacetype', 'hfiaasurcharge', 'federalpolicyfee',\n",
    "        'policycost', 'numberoffloorsininsuredbuilding', 'latitude', 'longitude',\n",
    "        'elevationdifference', 'lowestadjacentgrade', 'lowestfloorelevation',\n",
    "        'basefloodelevation', 'countycode', 'totalbuildinginsurancecoverage',\n",
    "        'totalcontentsinsurancecoverage', 'censustract'\n",
    "    ],\n",
    "    outputCol='predict_features_19'\n",
    ")\n",
    "\n",
    "new_df = assembler_f19.transform(new_df)\n",
    "\n",
    "# Select the required columns\n",
    "df_f19 = new_df.select('predict_features_19', 'totalinsurancepremiumofthepolicy')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data_f19, test_data_f19 = df_f19.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Define and fit the Linear Regression model\n",
    "lr_f19 = LinearRegression(\n",
    "    featuresCol='predict_features_19', \n",
    "    labelCol='totalinsurancepremiumofthepolicy', \n",
    "    predictionCol='predicted_premium_f19', \n",
    "    regParam=0.3\n",
    ")\n",
    "lr_model_f19 = lr_f19.fit(train_data_f19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ebeb16f7-e533-4802-8097-1b132ee0cd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 209:=====================================================> (94 + 3) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data for Linear Regression using 19 features: 117.5606982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 209:=====================================================> (95 + 2) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RMSE for Train data\n",
    "predictions_rmse_train_f19 = lr_model_f19.transform(train_data_f19)\n",
    "\n",
    "evaluator_f19 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f19', metricName = 'rmse')\n",
    "rmse_train_f19 = evaluator_f19.evaluate(predictions_rmse_train_f19)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data for Linear Regression using 19 features: {:.7f}\".format(rmse_train_f19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "faae9771-65ba-481b-a89a-a05312a9fb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 211:====================================================>  (92 + 5) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on train data for Linear Regression using 19 features: 0.9948655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 211:====================================================>  (93 + 4) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# R-squared for Train data\n",
    "evaluator_r2_f19 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f19', metricName = 'r2')\n",
    "r2_train_f19 = evaluator_r2_f19.evaluate(predictions_rmse_train_f19)\n",
    "print(\"R-squared (R2) on train data for Linear Regression using 19 features: {:.7f}\".format(r2_train_f19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2fcbac42-ecb9-43a0-82f3-6f75b4f817a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 213:====================================================>  (93 + 4) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Linear Regression using 19 features: 118.0065623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 213:=====================================================> (95 + 2) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RMSE for Test data\n",
    "predictions_rmse_test_f19 = lr_model_f19.transform(test_data_f19)\n",
    "\n",
    "evaluator_f19 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f19', metricName = 'rmse')\n",
    "rmse_test_f19 = evaluator_f19.evaluate(predictions_rmse_test_f19)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data for Linear Regression using 19 features: {:.7f}\".format(rmse_test_f19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c14faef-79df-489a-904f-409f68d00b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 215:====================================================>  (93 + 4) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data for Linear Regression using 19 features: 0.9950025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 215:=====================================================> (94 + 3) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# R-squared for Test data\n",
    "evaluator_r2_f19 = RegressionEvaluator(labelCol = 'totalinsurancepremiumofthepolicy', predictionCol = 'predicted_premium_f19', metricName = 'r2')\n",
    "r2_test_f19 = evaluator_r2_f19.evaluate(predictions_rmse_test_f19)\n",
    "print(\"R-squared (R2) on test data for Linear Regression using 19 features: {:.7f}\".format(r2_test_f19))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6030344-6e0e-46c8-ac37-13f55f69de0e",
   "metadata": {},
   "source": [
    "## Note to group: exclude Bar plots when submitting for Milestone 4. This is incorrect; not a Fitting Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb8d98-7c5a-4249-9701-c3e1f7325141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone 3: Bar plot with RMSE values, comparing train and test data\n",
    "# To answer Question 4: \"Where does your model fit in the fitting graph?\"\n",
    "df_barplot = pd.DataFrame({'RMSE': ['Train RMSE', 'Test RMSE'], 'Error Values': [118.133, 117.118]})  \n",
    "df_barplot.plot.bar(x = 'RMSE', y = 'Error Values')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f587a-87c6-4f2c-a769-7e2a71e3e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone 3: Bar plot with R-squared values, comparing train and test data\n",
    "# To answer Question 4: \"Where does your model fit in the fitting graph?\"\n",
    "df_barplot = pd.DataFrame({'R-Squared': ['Train R-Squared', 'Test R-Squared'], 'Values': [0.995, 0.995]})  \n",
    "df_barplot.plot.bar(x = 'R-Squared', y = 'Values')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f5b933-e369-4bee-b609-6d70597657ff",
   "metadata": {},
   "source": [
    "### Scatterplots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554a239-9402-4b17-901d-258733d8d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_num_df = changed_df.sample(False, 0.1)\n",
    "num_pdf = changed_num_df.toPandas()\n",
    "print(num_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b2830-1613-4da2-9cd1-060dc91ba3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_pdf['policycost'],y=num_pdf['totalinsurancepremiumofthepolicy'])\n",
    "plt.xlabel('Policy Cost')\n",
    "plt.ylabel('Total Insurance Premium of the Policy')          \n",
    "plt.title('Policy Cost vs Total Insurance Premium of the Policy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdeceeb-ca90-4ffb-9ca1-28125c9acfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=num_pdf['countycode'],y=num_pdf['censustract'])\n",
    "plt.xlabel('County Code')\n",
    "plt.ylabel('Census Tract') \n",
    "plt.title('County Code vs Census Tract')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eadc63-86f7-4727-9c8a-0bcd96481610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "plt.scatter(x=num_pdf['policycount'],y=num_pdf['totalbuildinginsurancecoverage'])\n",
    "plt.xlabel('Policy Count')\n",
    "plt.ylabel('Total Building Insurance Coverage') \n",
    "plt.title('Policy Count vs Total Building Insurance Coverage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23878e4",
   "metadata": {},
   "source": [
    "# Exploring Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7db42aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 13:18:02 WARN MemoryStore: Not enough space to cache rdd_99_88 in memory! (computed 1026.9 KiB so far)\n",
      "24/05/28 13:18:02 WARN MemoryStore: Not enough space to cache rdd_99_89 in memory! (computed 1026.9 KiB so far)\n",
      "24/05/28 13:18:02 WARN BlockManager: Persisting block rdd_99_89 to disk instead.\n",
      "24/05/28 13:18:02 WARN BlockManager: Persisting block rdd_99_88 to disk instead.\n",
      "24/05/28 13:18:02 WARN MemoryStore: Not enough space to cache rdd_99_91 in memory! (computed 2.3 MiB so far)\n",
      "24/05/28 13:18:02 WARN BlockManager: Persisting block rdd_99_91 to disk instead.\n",
      "24/05/28 13:18:02 WARN MemoryStore: Not enough space to cache rdd_99_90 in memory! (computed 1544.0 KiB so far)\n",
      "24/05/28 13:18:02 WARN BlockManager: Persisting block rdd_99_90 to disk instead.\n",
      "24/05/28 13:18:03 WARN MemoryStore: Not enough space to cache rdd_99_84 in memory! (computed 2.3 MiB so far)\n",
      "24/05/28 13:18:03 WARN MemoryStore: Not enough space to cache rdd_99_85 in memory! (computed 2.3 MiB so far)\n",
      "24/05/28 13:18:03 WARN BlockManager: Persisting block rdd_99_84 to disk instead.\n",
      "24/05/28 13:18:03 WARN BlockManager: Persisting block rdd_99_85 to disk instead.\n",
      "24/05/28 13:18:03 WARN MemoryStore: Not enough space to cache rdd_99_87 in memory! (computed 2.3 MiB so far)\n",
      "24/05/28 13:18:03 WARN BlockManager: Persisting block rdd_99_87 to disk instead.\n",
      "24/05/28 13:18:13 WARN MemoryStore: Not enough space to cache rdd_99_96 in memory! (computed 19.5 MiB so far)\n",
      "24/05/28 13:18:13 WARN BlockManager: Persisting block rdd_99_96 to disk instead.\n",
      "24/05/28 13:18:17 WARN MemoryStore: Not enough space to cache rdd_99_93 in memory! (computed 19.5 MiB so far)\n",
      "24/05/28 13:18:17 WARN MemoryStore: Not enough space to cache rdd_99_92 in memory! (computed 29.5 MiB so far)\n",
      "24/05/28 13:18:17 WARN BlockManager: Persisting block rdd_99_93 to disk instead.\n",
      "24/05/28 13:18:17 WARN BlockManager: Persisting block rdd_99_92 to disk instead.\n",
      "24/05/28 13:18:17 WARN MemoryStore: Not enough space to cache rdd_99_94 in memory! (computed 46.0 MiB so far)\n",
      "24/05/28 13:18:17 WARN BlockManager: Persisting block rdd_99_94 to disk instead.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# creating train and test sets\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['basefloodelevation',\n",
    "               'basementenclosurecrawlspacetype',\n",
    "               'censustract',\n",
    "               'countycode',\n",
    "               'crsdiscount',\n",
    "               'elevationdifference',\n",
    "               'federalpolicyfee',\n",
    "               'hfiaasurcharge',\n",
    "               'latitude',\n",
    "               'longitude',\n",
    "               'lowestadjacentgrade',\n",
    "               'lowestfloorelevation',\n",
    "               'numberoffloorsininsuredbuilding',\n",
    "               'occupancytype',\n",
    "               'policycost',\n",
    "               'policycount',\n",
    "               'policytermindicator',\n",
    "               'totalbuildinginsurancecoverage',\n",
    "               'totalcontentsinsurancecoverage'],\n",
    "    outputCol='new_features')\n",
    "\n",
    "new_df = assembler.transform(new_df)\n",
    "final_data = new_df.select('new_features', 'totalinsurancepremiumofthepolicy')\n",
    "\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol='new_features', labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium')\n",
    "dt_model = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f63eff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: (19,[0,4,5,7,11,14,15,17,18],[0.017533895627270216,0.01191545429335759,0.027226298854630335,0.004125763495195851,0.004570672369236896,0.7319316313274444,0.16124047562678448,0.019769204962302646,0.021686603443777562])\n",
      "Top 10 Feature Importances:\n",
      "  policycost: 0.732\n",
      "  policycount: 0.161\n",
      "  elevationdifference: 0.027\n",
      "  totalcontentsinsurancecoverage: 0.022\n",
      "  totalbuildinginsurancecoverage: 0.020\n",
      "  basefloodelevation: 0.018\n",
      "  crsdiscount: 0.012\n",
      "  lowestfloorelevation: 0.005\n",
      "  hfiaasurcharge: 0.004\n",
      "  basementenclosurecrawlspacetype: 0.000\n"
     ]
    }
   ],
   "source": [
    "# After fitting the DecisionTreeRegressor model\n",
    "feature_importances = dt_model.featureImportances\n",
    "print(\"Feature Importances: {}\".format(feature_importances))\n",
    "\n",
    "# Constructing Feature Importance to find top 10 features\n",
    "feature_importance_list = list(zip(assembler.getInputCols(), feature_importances))\n",
    "feature_importance_list_sorted = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 Feature Importances:\")\n",
    "for feature, importance in feature_importance_list_sorted[:10]:\n",
    "    print(\"  {}: {:.3f}\".format(feature, importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddabe6a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 19 Feature Importances:\n",
      "  policycost: 0.732\n",
      "  policycount: 0.161\n",
      "  elevationdifference: 0.027\n",
      "  totalcontentsinsurancecoverage: 0.022\n",
      "  totalbuildinginsurancecoverage: 0.020\n",
      "  basefloodelevation: 0.018\n",
      "  crsdiscount: 0.012\n",
      "  lowestfloorelevation: 0.005\n",
      "  hfiaasurcharge: 0.004\n",
      "  basementenclosurecrawlspacetype: 0.000\n",
      "  censustract: 0.000\n",
      "  countycode: 0.000\n",
      "  federalpolicyfee: 0.000\n",
      "  latitude: 0.000\n",
      "  longitude: 0.000\n",
      "  lowestadjacentgrade: 0.000\n",
      "  numberoffloorsininsuredbuilding: 0.000\n",
      "  occupancytype: 0.000\n",
      "  policytermindicator: 0.000\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 19 Feature Importances:\")\n",
    "for feature, importance in feature_importance_list_sorted[:19]:\n",
    "    print(\"  {}: {:.3f}\".format(feature, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1e3dc",
   "metadata": {},
   "source": [
    "# Construct 5 Decision Trees Models to find the Most Ideal Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ef441",
   "metadata": {},
   "source": [
    "### Using the Feature Importance model information, will start by training a Decision Trees Model with 1 feature (Top Feature Importance), then 5 features, 10 features, 15 features, and finally 19 features. There are a total 20 numerical features in the FEMA dataset, and the totalinsurancepremiumofthepolicy feature is the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2040a8",
   "metadata": {},
   "source": [
    "### 1. Decision Trees Model with 1 Feature: \n",
    "policycost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99c852eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data for Decision Tree using 1 feature: 1292.9460710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on train data for Decision Tree using 1 feature: 0.3869786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Decision Tree using 1 feature: 1280.3065343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=====================================================>  (92 + 5) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data for Decision Tree using 1 feature: 0.3938157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:======================================================> (95 + 2) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler_f1 = VectorAssembler(\n",
    "    inputCols=['policycost'], outputCol='predict_features_1')\n",
    "\n",
    "new_df = assembler_f1.transform(new_df)\n",
    "\n",
    "df_f1 = new_df.select('predict_features_1', 'totalinsurancepremiumofthepolicy')\n",
    "train_data_f1, test_data_f1 = df_f1.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "dt_f1 = DecisionTreeRegressor(featuresCol='predict_features_1', labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f1')\n",
    "dt_model_f1 = dt_f1.fit(train_data_f1)\n",
    "\n",
    "# RMSE for Train data\n",
    "predictions_rmse_train_f1 = dt_model_f1.transform(train_data_f1)\n",
    "\n",
    "evaluator_f1 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f1', metricName='rmse')\n",
    "rmse_train_f1 = evaluator_f1.evaluate(predictions_rmse_train_f1)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data for Decision Tree using 1 feature: {:.7f}\".format(rmse_train_f1))\n",
    "\n",
    "# R-squared for Train data\n",
    "evaluator_r2_f1 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f1', metricName='r2')\n",
    "r2_train_f1 = evaluator_r2_f1.evaluate(predictions_rmse_train_f1)\n",
    "print(\"R-squared (R2) on train data for Decision Tree using 1 feature: {:.7f}\".format(r2_train_f1))\n",
    "\n",
    "# RMSE for Test data\n",
    "predictions_rmse_test_f1 = dt_model_f1.transform(test_data_f1)\n",
    "\n",
    "evaluator_f1 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f1', metricName='rmse')\n",
    "rmse_test_f1 = evaluator_f1.evaluate(predictions_rmse_test_f1)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data for Decision Tree using 1 feature: {:.7f}\".format(rmse_test_f1))\n",
    "\n",
    "# R-squared for Test data\n",
    "evaluator_r2_f1 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f1', metricName='r2')\n",
    "r2_test_f1 = evaluator_r2_f1.evaluate(predictions_rmse_test_f1)\n",
    "print(\"R-squared (R2) on test data for Decision Tree using 1 feature: {:.7f}\".format(r2_test_f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bd72e5",
   "metadata": {},
   "source": [
    "### 2. Decision Trees Model with 5 Feature: \n",
    "policycost, policycount, elevationdifference, totalcontentsinsurancecoverage,totalbuildinginsurancecoverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbb9caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data for Decision Tree using 5 features: 1152.8592878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on train data for Decision Tree using 5 features: 0.5126214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Decision Tree using 5 features: 1137.6243066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:=======================================================>(96 + 1) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data for Decision Tree using 5 features: 0.5213957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler_f5 = VectorAssembler(\n",
    "    inputCols=['policycost',\n",
    "               'policycount',\n",
    "               'elevationdifference',\n",
    "               'totalcontentsinsurancecoverage',\n",
    "               'totalbuildinginsurancecoverage'], outputCol='predict_features_5')\n",
    "\n",
    "new_df = assembler_f5.transform(new_df)\n",
    "\n",
    "df_f5 = new_df.select('predict_features_5', 'totalinsurancepremiumofthepolicy')\n",
    "train_data_f5, test_data_f5 = df_f5.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "dt_f5 = DecisionTreeRegressor(featuresCol='predict_features_5', labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f5')\n",
    "dt_model_f5 = dt_f5.fit(train_data_f5)\n",
    "\n",
    "# RMSE for Train data\n",
    "predictions_rmse_train_f5 = dt_model_f5.transform(train_data_f5)\n",
    "\n",
    "evaluator_f5 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f5', metricName='rmse')\n",
    "rmse_train_f5 = evaluator_f5.evaluate(predictions_rmse_train_f5)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data for Decision Tree using 5 features: {:.7f}\".format(rmse_train_f5))\n",
    "\n",
    "# R-squared for Train data\n",
    "evaluator_r2_f5 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f5', metricName='r2')\n",
    "r2_train_f5 = evaluator_r2_f5.evaluate(predictions_rmse_train_f5)\n",
    "print(\"R-squared (R2) on train data for Decision Tree using 5 features: {:.7f}\".format(r2_train_f5))\n",
    "\n",
    "# RMSE for Test data\n",
    "predictions_rmse_test_f5 = dt_model_f5.transform(test_data_f5)\n",
    "\n",
    "evaluator_f5 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f5', metricName='rmse')\n",
    "rmse_test_f5 = evaluator_f5.evaluate(predictions_rmse_test_f5)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data for Decision Tree using 5 features: {:.7f}\".format(rmse_test_f5))\n",
    "\n",
    "# R-squared for Test data\n",
    "evaluator_r2_f5 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f5', metricName='r2')\n",
    "r2_test_f5 = evaluator_r2_f5.evaluate(predictions_rmse_test_f5)\n",
    "print(\"R-squared (R2) on test data for Decision Tree using 5 features: {:.7f}\".format(r2_test_f5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e18894",
   "metadata": {},
   "source": [
    "### 3. Decision Trees Model with 10 Feature: \n",
    "policycost, policycount, elevationdifference, totalcontentsinsurancecoverage, totalbuildinginsurancecoverage, basefloodelevation, crsdiscount, lowestfloorelevation, hfiaasurcharge, basementenclosurecrawlspacetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e87f7408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data for Decision Tree using 10 features: 1141.8630778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on train data for Decision Tree using 10 features: 0.5219198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Decision Tree using 10 features: 1129.1083224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:====================================================>  (92 + 5) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data for Decision Tree using 10 features: 0.5284291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 102:=====================================================> (95 + 2) / 97]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler_f10 = VectorAssembler(\n",
    "    inputCols=['policycost',\n",
    "               'policycount',\n",
    "               'elevationdifference',\n",
    "               'totalcontentsinsurancecoverage',\n",
    "               'totalbuildinginsurancecoverage',\n",
    "               'basefloodelevation',\n",
    "               'crsdiscount',\n",
    "               'lowestfloorelevation',\n",
    "               'hfiaasurcharge',\n",
    "               'basementenclosurecrawlspacetype'], outputCol='predict_features_10')\n",
    "\n",
    "new_df = assembler_f10.transform(new_df)\n",
    "\n",
    "df_f10 = new_df.select('predict_features_10', 'totalinsurancepremiumofthepolicy')\n",
    "train_data_f10, test_data_f10 = df_f10.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "dt_f10 = DecisionTreeRegressor(featuresCol='predict_features_10', labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f10')\n",
    "dt_model_f10 = dt_f10.fit(train_data_f10)\n",
    "\n",
    "# RMSE for Train data\n",
    "predictions_rmse_train_f10 = dt_model_f10.transform(train_data_f10)\n",
    "\n",
    "evaluator_f10 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f10', metricName='rmse')\n",
    "rmse_train_f10 = evaluator_f10.evaluate(predictions_rmse_train_f10)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data for Decision Tree using 10 features: {:.7f}\".format(rmse_train_f10))\n",
    "\n",
    "# R-squared for Train data\n",
    "evaluator_r2_f10 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f10', metricName='r2')\n",
    "r2_train_f10 = evaluator_r2_f10.evaluate(predictions_rmse_train_f10)\n",
    "print(\"R-squared (R2) on train data for Decision Tree using 10 features: {:.7f}\".format(r2_train_f10))\n",
    "\n",
    "# RMSE for Test data\n",
    "predictions_rmse_test_f10 = dt_model_f10.transform(test_data_f10)\n",
    "\n",
    "evaluator_f10 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f10', metricName='rmse')\n",
    "rmse_test_f10 = evaluator_f10.evaluate(predictions_rmse_test_f10)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data for Decision Tree using 10 features: {:.7f}\".format(rmse_test_f10))\n",
    "\n",
    "# R-squared for Test data\n",
    "evaluator_r2_f10 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f10', metricName='r2')\n",
    "r2_test_f10 = evaluator_r2_f10.evaluate(predictions_rmse_test_f10)\n",
    "print(\"R-squared (R2) on test data for Decision Tree using 10 features: {:.7f}\".format(r2_test_f10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd626942",
   "metadata": {},
   "source": [
    "### 4. Decision Trees Model with 15 Feature:\n",
    "policycost, policycount, elevationdifference, totalcontentsinsurancecoverage, totalbuildinginsurancecoverage, basefloodelevation, crsdiscount, lowestfloorelevation, hfiaasurcharge, basementenclosurecrawlspacetype, censustract, countycode, federalpolicyfee, latitude, longitude\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18f44699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 13:53:11 WARN MemoryStore: Not enough space to cache rdd_367_89 in memory! (computed 41.6 MiB so far)\n",
      "24/05/28 13:53:11 WARN MemoryStore: Not enough space to cache rdd_367_91 in memory! (computed 17.6 MiB so far)\n",
      "24/05/28 13:53:11 WARN BlockManager: Persisting block rdd_367_89 to disk instead.\n",
      "24/05/28 13:53:11 WARN BlockManager: Persisting block rdd_367_91 to disk instead.\n",
      "24/05/28 13:53:11 WARN MemoryStore: Not enough space to cache rdd_367_94 in memory! (computed 41.6 MiB so far)\n",
      "24/05/28 13:53:11 WARN BlockManager: Persisting block rdd_367_94 to disk instead.\n",
      "24/05/28 13:53:11 WARN MemoryStore: Not enough space to cache rdd_367_95 in memory! (computed 26.6 MiB so far)\n",
      "24/05/28 13:53:11 WARN BlockManager: Persisting block rdd_367_95 to disk instead.\n",
      "24/05/28 13:53:11 WARN MemoryStore: Not enough space to cache rdd_367_92 in memory! (computed 41.6 MiB so far)\n",
      "24/05/28 13:53:11 WARN BlockManager: Persisting block rdd_367_92 to disk instead.\n",
      "24/05/28 13:53:11 WARN MemoryStore: Not enough space to cache rdd_367_90 in memory! (computed 26.6 MiB so far)\n",
      "24/05/28 13:53:11 WARN BlockManager: Persisting block rdd_367_90 to disk instead.\n",
      "24/05/28 13:53:11 WARN MemoryStore: Not enough space to cache rdd_367_93 in memory! (computed 41.6 MiB so far)\n",
      "24/05/28 13:53:11 WARN BlockManager: Persisting block rdd_367_93 to disk instead.\n",
      "24/05/28 13:53:11 WARN MemoryStore: Not enough space to cache rdd_367_88 in memory! (computed 41.6 MiB so far)\n",
      "24/05/28 13:53:11 WARN BlockManager: Persisting block rdd_367_88 to disk instead.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data for Decision Tree using 15 features: 1146.1381342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on train data for Decision Tree using 15 features: 0.5182932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Decision Tree using 15 features: 1132.0465987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 124:=====================================================> (94 + 3) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data for Decision Tree using 15 features: 0.5260645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler_f15 = VectorAssembler(\n",
    "    inputCols=['policycost',\n",
    "               'policycount',\n",
    "               'elevationdifference',\n",
    "               'totalcontentsinsurancecoverage',\n",
    "               'totalbuildinginsurancecoverage',\n",
    "               'basefloodelevation',\n",
    "               'crsdiscount',\n",
    "               'lowestfloorelevation',\n",
    "               'hfiaasurcharge',\n",
    "               'basementenclosurecrawlspacetype',\n",
    "               'censustract',\n",
    "               'countycode',\n",
    "               'federalpolicyfee',\n",
    "               'latitude',\n",
    "               'longitude'], outputCol='predict_features_15')\n",
    "\n",
    "new_df = assembler_f15.transform(new_df)\n",
    "\n",
    "df_f15 = new_df.select('predict_features_15', 'totalinsurancepremiumofthepolicy')\n",
    "train_data_f15, test_data_f15 = df_f15.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "dt_f15 = DecisionTreeRegressor(featuresCol='predict_features_15', labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f15')\n",
    "dt_model_f15 = dt_f15.fit(train_data_f15)\n",
    "\n",
    "# RMSE for Train data\n",
    "predictions_rmse_train_f15 = dt_model_f15.transform(train_data_f15)\n",
    "\n",
    "evaluator_f15 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f15', metricName='rmse')\n",
    "rmse_train_f15 = evaluator_f15.evaluate(predictions_rmse_train_f15)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data for Decision Tree using 15 features: {:.7f}\".format(rmse_train_f15))\n",
    "\n",
    "# R-squared for Train data\n",
    "evaluator_r2_f15 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f15', metricName='r2')\n",
    "r2_train_f15 = evaluator_r2_f15.evaluate(predictions_rmse_train_f15)\n",
    "print(\"R-squared (R2) on train data for Decision Tree using 15 features: {:.7f}\".format(r2_train_f15))\n",
    "\n",
    "# RMSE for Test data\n",
    "predictions_rmse_test_f15 = dt_model_f15.transform(test_data_f15)\n",
    "\n",
    "evaluator_f15 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f15', metricName='rmse')\n",
    "rmse_test_f15 = evaluator_f15.evaluate(predictions_rmse_test_f15)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data for Decision Tree using 15 features: {:.7f}\".format(rmse_test_f15))\n",
    "\n",
    "# R-squared for Test data\n",
    "evaluator_r2_f15 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f15', metricName='r2')\n",
    "r2_test_f15 = evaluator_r2_f15.evaluate(predictions_rmse_test_f15)\n",
    "print(\"R-squared (R2) on test data for Decision Tree using 15 features: {:.7f}\".format(r2_test_f15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343a04a",
   "metadata": {},
   "source": [
    "### 5. Decision Trees Model with 19 Feature:\n",
    "policycost, policycount, elevationdifference, totalcontentsinsurancecoverage, totalbuildinginsurancecoverage, basefloodelevation, crsdiscount, lowestfloorelevation, hfiaasurcharge, basementenclosurecrawlspacetype, censustract, countycode, federalpolicyfee, latitude, longitude, lowestadjacentgrade, numberoffloorsininsuredbuilding, occupancytype, policytermindicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7b95bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 14:03:48 WARN MemoryStore: Not enough space to cache rdd_446_86 in memory! (computed 29.5 MiB so far)\n",
      "24/05/28 14:03:48 WARN MemoryStore: Not enough space to cache rdd_446_84 in memory! (computed 29.5 MiB so far)\n",
      "24/05/28 14:03:48 WARN BlockManager: Persisting block rdd_446_84 to disk instead.\n",
      "24/05/28 14:03:48 WARN BlockManager: Persisting block rdd_446_86 to disk instead.\n",
      "24/05/28 14:03:48 WARN MemoryStore: Not enough space to cache rdd_446_82 in memory! (computed 29.5 MiB so far)\n",
      "24/05/28 14:03:48 WARN BlockManager: Persisting block rdd_446_82 to disk instead.\n",
      "24/05/28 14:03:48 WARN MemoryStore: Not enough space to cache rdd_446_87 in memory! (computed 29.5 MiB so far)\n",
      "24/05/28 14:03:48 WARN BlockManager: Persisting block rdd_446_87 to disk instead.\n",
      "24/05/28 14:03:48 WARN MemoryStore: Not enough space to cache rdd_446_85 in memory! (computed 29.5 MiB so far)\n",
      "24/05/28 14:03:48 WARN BlockManager: Persisting block rdd_446_85 to disk instead.\n",
      "24/05/28 14:03:48 WARN MemoryStore: Not enough space to cache rdd_446_81 in memory! (computed 29.5 MiB so far)\n",
      "24/05/28 14:03:48 WARN BlockManager: Persisting block rdd_446_81 to disk instead.\n",
      "24/05/28 14:03:48 WARN MemoryStore: Not enough space to cache rdd_446_80 in memory! (computed 29.5 MiB so far)\n",
      "24/05/28 14:03:48 WARN BlockManager: Persisting block rdd_446_80 to disk instead.\n",
      "24/05/28 14:03:48 WARN MemoryStore: Not enough space to cache rdd_446_83 in memory! (computed 29.5 MiB so far)\n",
      "24/05/28 14:03:48 WARN BlockManager: Persisting block rdd_446_83 to disk instead.\n",
      "24/05/28 14:04:05 WARN MemoryStore: Not enough space to cache rdd_446_92 in memory! (computed 19.5 MiB so far)\n",
      "24/05/28 14:04:05 WARN BlockManager: Persisting block rdd_446_92 to disk instead.\n",
      "24/05/28 14:04:05 WARN MemoryStore: Not enough space to cache rdd_446_90 in memory! (computed 46.0 MiB so far)\n",
      "24/05/28 14:04:05 WARN BlockManager: Persisting block rdd_446_90 to disk instead.\n",
      "24/05/28 14:04:06 WARN MemoryStore: Not enough space to cache rdd_446_88 in memory! (computed 19.5 MiB so far)\n",
      "24/05/28 14:04:06 WARN BlockManager: Persisting block rdd_446_88 to disk instead.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on train data for Decision Tree using 19 features: 1142.1079771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on train data for Decision Tree using 19 features: 0.5216784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Decision Tree using 19 features: 1127.5757127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 146:======================================================>(96 + 1) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data for Decision Tree using 19 features: 0.5297924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler_f19 = VectorAssembler(\n",
    "    inputCols=['policycost',\n",
    "               'policycount',\n",
    "               'elevationdifference',\n",
    "               'totalcontentsinsurancecoverage',\n",
    "               'totalbuildinginsurancecoverage',\n",
    "               'basefloodelevation',\n",
    "               'crsdiscount',\n",
    "               'lowestfloorelevation',\n",
    "               'hfiaasurcharge',\n",
    "               'basementenclosurecrawlspacetype',\n",
    "               'censustract',\n",
    "               'countycode',\n",
    "               'federalpolicyfee',\n",
    "               'latitude',\n",
    "               'longitude',\n",
    "               'lowestadjacentgrade',\n",
    "               'numberoffloorsininsuredbuilding',\n",
    "               'occupancytype',\n",
    "               'policytermindicator'], outputCol='predict_features_19')\n",
    "\n",
    "new_df = assembler_f19.transform(new_df)\n",
    "\n",
    "df_f19 = new_df.select('predict_features_19', 'totalinsurancepremiumofthepolicy')\n",
    "train_data_f19, test_data_f19 = df_f19.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "dt_f19 = DecisionTreeRegressor(featuresCol='predict_features_19', labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f19')\n",
    "dt_model_f19 = dt_f19.fit(train_data_f19)\n",
    "\n",
    "# RMSE for Train data\n",
    "predictions_rmse_train_f19 = dt_model_f19.transform(train_data_f19)\n",
    "\n",
    "evaluator_f19 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f19', metricName='rmse')\n",
    "rmse_train_f19 = evaluator_f19.evaluate(predictions_rmse_train_f19)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data for Decision Tree using 19 features: {:.7f}\".format(rmse_train_f19))\n",
    "\n",
    "# R-squared for Train data\n",
    "evaluator_r2_f19 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f19', metricName='r2')\n",
    "r2_train_f19 = evaluator_r2_f19.evaluate(predictions_rmse_train_f19)\n",
    "print(\"R-squared (R2) on train data for Decision Tree using 19 features: {:.7f}\".format(r2_train_f19))\n",
    "\n",
    "# RMSE for Test data\n",
    "predictions_rmse_test_f19 = dt_model_f19.transform(test_data_f19)\n",
    "\n",
    "evaluator_f19 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f19', metricName='rmse')\n",
    "rmse_test_f19 = evaluator_f19.evaluate(predictions_rmse_test_f19)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data for Decision Tree using 19 features: {:.7f}\".format(rmse_test_f19))\n",
    "\n",
    "# R-squared for Test data\n",
    "evaluator_r2_f19 = RegressionEvaluator(labelCol='totalinsurancepremiumofthepolicy', predictionCol='predicted_premium_f19', metricName='r2')\n",
    "r2_test_f19 = evaluator_r2_f19.evaluate(predictions_rmse_test_f19)\n",
    "print(\"R-squared (R2) on test data for Decision Tree using 19 features: {:.7f}\".format(r2_test_f19))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05cd6bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been saved to 'model_comparison_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "results = [\n",
    "    ['Linear Regression', 'predict_features_1', 1643.9153, 0.0035, 1652.3239, 0.0034],\n",
    "    ['Linear Regression', 'predict_features_5', 1407.3977, 0.2815, 1375.8440, 0.2816],\n",
    "    ['Linear Regression', 'predict_features_10', 118.1326, 0.9949, 117.1185, 0.9949],\n",
    "    ['Linear Regression', 'predict_features_15', 117.6226, 0.9949, 118.2676, 0.9949],\n",
    "    ['Linear Regression', 'predict_features_19', 117.5607, 0.9949, 118.0066, 0.9950],\n",
    "    ['Decision Tree', 'predict_features_1', 1292.9461, 0.3870, 1280.3065, 0.3938],\n",
    "    ['Decision Tree', 'predict_features_5', 1152.8593, 0.5126, 1137.6243, 0.5214],\n",
    "    ['Decision Tree', 'predict_features_10', 1141.8631, 0.5219, 1129.1083, 0.5284],\n",
    "    ['Decision Tree', 'predict_features_15', 1146.1381, 0.5183, 1132.0466, 0.5261],\n",
    "    ['Decision Tree', 'predict_features_19', 1142.1080, 0.5217, 1127.5757, 0.5298]\n",
    "]\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Features', 'Train RMSE', 'Train R2', 'Test RMSE', 'Test R2'])\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "results_df.to_excel('model_comparison_results.xlsx', index=False)\n",
    "\n",
    "print(\"Results have been saved to 'model_comparison_results.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afff6d",
   "metadata": {},
   "source": [
    "# Interactive Map - Showcasing Geography, Flood Risk and Insurance Premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "18911cae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 113:======================================================>(96 + 1) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A23\n",
      "V24\n",
      "AOB\n",
      "A01\n",
      "A19\n",
      "AHB\n",
      "A16\n",
      "V19\n",
      "A21\n",
      "A17\n",
      "A18\n",
      "V27\n",
      "A22\n",
      "V06\n",
      "V22\n",
      "V21\n",
      "V08\n",
      "B\n",
      "V14\n",
      "V11\n",
      "V18\n",
      "V05\n",
      "V16\n",
      "A11\n",
      "A27\n",
      "A99\n",
      "V\n",
      "V04\n",
      "A09\n",
      "D\n",
      "V23\n",
      "A03\n",
      "VE\n",
      "V20\n",
      "A25\n",
      "V07\n",
      "C\n",
      "A26\n",
      "V02\n",
      "V15\n",
      "AE\n",
      "A10\n",
      "A\n",
      "V13\n",
      "A20\n",
      "X\n",
      "A28\n",
      "V09\n",
      "V03\n",
      "V12\n",
      "A15\n",
      "A30\n",
      "V17\n",
      "A14\n",
      "A24\n",
      "A0B\n",
      "A08\n",
      "V01\n",
      "A06\n",
      "AH\n",
      "A07\n",
      "A12\n",
      "V30\n",
      "V10\n",
      "A04\n",
      "A02\n",
      "AR\n",
      "A13\n",
      "A05\n",
      "AO\n",
      "A29\n",
      "A00\n",
      "V29\n",
      "V28\n",
      "ALT\n",
      "*\n",
      "V8\n",
      "A0\n",
      "EMG\n",
      "V9\n",
      "ARE\n",
      "X 0\n",
      "A E\n",
      "AO8\n",
      "E\n",
      "00X\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Understading flood zone category\n",
    "\n",
    "unique_floodzones = df.select(\"floodzone\").distinct().collect()\n",
    "for row in unique_floodzones:\n",
    "    print(row['floodzone'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1a9837df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|floodzone|risk_category|\n",
      "+---------+-------------+\n",
      "|        X|     Low Risk|\n",
      "|       AE|    High Risk|\n",
      "|        X|     Low Risk|\n",
      "|       AE|    High Risk|\n",
      "|      A10|    High Risk|\n",
      "|      A02|    High Risk|\n",
      "|      A06|    High Risk|\n",
      "|        X|     Low Risk|\n",
      "|        X|     Low Risk|\n",
      "|        X|     Low Risk|\n",
      "|      A08|    High Risk|\n",
      "|       AE|    High Risk|\n",
      "|       AE|    High Risk|\n",
      "|       AE|    High Risk|\n",
      "|        C|Moderate Risk|\n",
      "|        X|     Low Risk|\n",
      "|       AE|    High Risk|\n",
      "|        C|Moderate Risk|\n",
      "|        B|     Low Risk|\n",
      "|        X|     Low Risk|\n",
      "+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the new column with risk categories base of flood zone. \n",
    "# Reference: https://www.floodsmart.gov/flood-zones-and-maps\n",
    "\n",
    "df = df.withColumn('risk_category',\n",
    "                   F.when(F.col('floodzone').startswith('B'), 'Low Risk')\n",
    "                    .when(F.col('floodzone').startswith('X'), 'Low Risk')\n",
    "                    .when(F.col('floodzone').startswith('C'), 'Moderate Risk')\n",
    "                    .when(F.col('floodzone').startswith('A'), 'High Risk')\n",
    "                    .when(F.col('floodzone').startswith('V'), 'Coastal High Risk')\n",
    "                    .when(F.col('floodzone').startswith('VE'), 'Coastal High Risk')\n",
    "                    .when(F.col('floodzone').startswith('D'), 'Undetermined Risk')\n",
    "                    .otherwise('Unknown Risk'))\n",
    "\n",
    "# Show the result\n",
    "df.select('floodzone', 'risk_category').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43785aac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- agriculturestructureindicator: string (nullable = true)\n",
      " |-- basefloodelevation: double (nullable = true)\n",
      " |-- basementenclosurecrawlspacetype: integer (nullable = true)\n",
      " |-- cancellationdateoffloodpolicy: date (nullable = true)\n",
      " |-- censustract: long (nullable = true)\n",
      " |-- condominiumindicator: string (nullable = true)\n",
      " |-- construction: string (nullable = true)\n",
      " |-- countycode: integer (nullable = true)\n",
      " |-- crsdiscount: double (nullable = true)\n",
      " |-- deductibleamountinbuildingcoverage: string (nullable = true)\n",
      " |-- deductibleamountincontentscoverage: string (nullable = true)\n",
      " |-- elevatedbuildingindicator: string (nullable = true)\n",
      " |-- elevationcertificateindicator: string (nullable = true)\n",
      " |-- elevationdifference: integer (nullable = true)\n",
      " |-- federalpolicyfee: integer (nullable = true)\n",
      " |-- floodzone: string (nullable = true)\n",
      " |-- hfiaasurcharge: integer (nullable = true)\n",
      " |-- houseofworshipindicator: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- locationofcontents: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- lowestadjacentgrade: double (nullable = true)\n",
      " |-- lowestfloorelevation: double (nullable = true)\n",
      " |-- nonprofitindicator: string (nullable = true)\n",
      " |-- numberoffloorsininsuredbuilding: integer (nullable = true)\n",
      " |-- obstructiontype: string (nullable = true)\n",
      " |-- occupancytype: integer (nullable = true)\n",
      " |-- originalconstructiondate: date (nullable = true)\n",
      " |-- originalnbdate: date (nullable = true)\n",
      " |-- policycost: integer (nullable = true)\n",
      " |-- policycount: integer (nullable = true)\n",
      " |-- policyeffectivedate: date (nullable = true)\n",
      " |-- policyterminationdate: date (nullable = true)\n",
      " |-- policytermindicator: integer (nullable = true)\n",
      " |-- postfirmconstructionindicator: string (nullable = true)\n",
      " |-- primaryresidenceindicator: string (nullable = true)\n",
      " |-- propertystate: string (nullable = true)\n",
      " |-- reportedzipcode: string (nullable = true)\n",
      " |-- ratemethod: string (nullable = true)\n",
      " |-- regularemergencyprogramindicator: string (nullable = true)\n",
      " |-- reportedcity: string (nullable = true)\n",
      " |-- smallbusinessindicatorbuilding: string (nullable = true)\n",
      " |-- totalbuildinginsurancecoverage: integer (nullable = true)\n",
      " |-- totalcontentsinsurancecoverage: integer (nullable = true)\n",
      " |-- totalinsurancepremiumofthepolicy: integer (nullable = true)\n",
      " |-- risk_category: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3efa95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for the interactive map\n",
    "df_map = df.select(\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'floodzone',\n",
    "    'risk_category',\n",
    "    'totalinsurancepremiumofthepolicy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f3cc594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- floodzone: string (nullable = true)\n",
      " |-- risk_category: string (nullable = false)\n",
      " |-- totalinsurancepremiumofthepolicy: integer (nullable = true)\n",
      "\n",
      "+--------+---------+---------+-------------+--------------------------------+\n",
      "|latitude|longitude|floodzone|risk_category|totalinsurancepremiumofthepolicy|\n",
      "+--------+---------+---------+-------------+--------------------------------+\n",
      "|43.3    |-71.8    |X        |Low Risk     |375                             |\n",
      "|30.5    |-91.0    |AE       |High Risk    |280                             |\n",
      "|33.7    |-79.0    |X        |Low Risk     |335                             |\n",
      "|34.0    |-86.0    |AE       |High Risk    |916                             |\n",
      "|26.0    |-80.1    |A10      |High Risk    |1288                            |\n",
      "|32.8    |-80.0    |A02      |High Risk    |489                             |\n",
      "|39.7    |-74.2    |A06      |High Risk    |1372                            |\n",
      "|29.6    |-95.1    |X        |Low Risk     |335                             |\n",
      "|43.1    |-70.9    |X        |Low Risk     |1403                            |\n",
      "|28.0    |-82.6    |X        |Low Risk     |419                             |\n",
      "+--------+---------+---------+-------------+--------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[latitude: double, longitude: double, floodzone: string, risk_category: string, totalinsurancepremiumofthepolicy: int]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the new DataFrame schema and some data\n",
    "df_map.printSchema()\n",
    "df_map.show(10, truncate=False)\n",
    "\n",
    "# Optionally, cache the new DataFrame if you plan to use it multiple times\n",
    "df_map.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9a27174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting folium\n",
      "  Downloading folium-0.16.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 4.0 MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting branca>=0.6.0\n",
      "  Downloading branca-0.7.2-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numpy in /Users/mengkong/opt/anaconda3/lib/python3.9/site-packages (from folium) (1.21.5)\n",
      "Requirement already satisfied: jinja2>=2.9 in /Users/mengkong/opt/anaconda3/lib/python3.9/site-packages (from folium) (2.11.3)\n",
      "Requirement already satisfied: requests in /Users/mengkong/opt/anaconda3/lib/python3.9/site-packages (from folium) (2.27.1)\n",
      "Collecting xyzservices\n",
      "  Downloading xyzservices-2024.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 603 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2>=2.9\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/mengkong/opt/anaconda3/lib/python3.9/site-packages (from jinja2>=2.9->folium) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mengkong/opt/anaconda3/lib/python3.9/site-packages (from requests->folium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mengkong/opt/anaconda3/lib/python3.9/site-packages (from requests->folium) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mengkong/opt/anaconda3/lib/python3.9/site-packages (from requests->folium) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mengkong/opt/anaconda3/lib/python3.9/site-packages (from requests->folium) (2.0.4)\n",
      "Installing collected packages: jinja2, xyzservices, branca, folium\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\u001b[0m\n",
      "Successfully installed branca-0.7.2 folium-0.16.0 jinja2-3.1.4 xyzservices-2024.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Assuming df_map is already available as a DataFrame with relevant columns\n",
    "df_map_pd = df_map.toPandas()\n",
    "\n",
    "# Create a map centered around the average latitude and longitude\n",
    "map_center = [df_map_pd['latitude'].mean(), df_map_pd['longitude'].mean()]\n",
    "interactive_map = folium.Map(location=map_center, zoom_start=6)\n",
    "\n",
    "# Create a marker cluster\n",
    "marker_cluster = MarkerCluster().add_to(interactive_map)\n",
    "\n",
    "# Function to determine marker color based on risk category\n",
    "def get_marker_color(risk_category):\n",
    "    if risk_category == 'Low Risk':\n",
    "        return 'green'\n",
    "    elif risk_category == 'Moderate Risk':\n",
    "        return 'blue'\n",
    "    elif risk_category == 'High Risk':\n",
    "        return 'orange'\n",
    "    elif risk_category == 'Coastal High Risk':\n",
    "        return 'red'\n",
    "    elif risk_category == 'Undetermined Risk':\n",
    "        return 'gray'\n",
    "    else:\n",
    "        return 'black'\n",
    "\n",
    "# Add markers to the map\n",
    "for idx, row in df_map_pd.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=f\"Flood Zone: {row['floodzone']}<br>Risk Category: {row['risk_category']}<br>Insurance Premium: ${row['totalinsurancepremiumofthepolicy']}\",\n",
    "        icon=folium.Icon(color=get_marker_color(row['risk_category']))\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "interactive_map.save('interactive_map.html')\n",
    "\n",
    "# Display the map\n",
    "interactive_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab89bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
